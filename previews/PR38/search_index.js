var documenterSearchIndex = {"docs":
[{"location":"gp.html#Gaussian-Process-Library","page":"Gaussian Process Library","title":"Gaussian Process Library","text":"","category":"section"},{"location":"gp.html","page":"Gaussian Process Library","title":"Gaussian Process Library","text":"Pages = [\"gp.md\"]","category":"page"},{"location":"gp.html","page":"Gaussian Process Library","title":"Gaussian Process Library","text":"This section describes a library for Gaussian process time series models. A technical overview of key concepts can be found in the following references.","category":"page"},{"location":"gp.html","page":"Gaussian Process Library","title":"Gaussian Process Library","text":"Roberts S, Osborne M, Ebden M, Reece S, Gibson N, Aigrain S. 2013. Gaussian processes for time-series modelling. Phil Trans R Soc A 371: 20110550. http://dx.doi.org/10.1098/rsta.2011.0550","category":"page"},{"location":"gp.html","page":"Gaussian Process Library","title":"Gaussian Process Library","text":"Rasmussen C, Williams C. 2006. Gaussian Processes for Machine Learning. MIT Press, Cambridge, MA. http://gaussianprocess.org/gpml/chapters/","category":"page"},{"location":"gp.html#AutoGP.GP","page":"Gaussian Process Library","title":"AutoGP.GP","text":"Module for Gaussian process modeling library.\n\n\n\n\n\n","category":"module"},{"location":"gp.html#gp_cov_kernel","page":"Gaussian Process Library","title":"Covariance Kernels","text":"","category":"section"},{"location":"gp.html#AutoGP.GP.Node","page":"Gaussian Process Library","title":"AutoGP.GP.Node","text":"abstract type Node end\n\nAbstract class for covariance kernels.\n\n\n\n\n\n","category":"type"},{"location":"gp.html#AutoGP.GP.LeafNode","page":"Gaussian Process Library","title":"AutoGP.GP.LeafNode","text":"abstract type LeafNode <: Node end\n\nAbstract class for primitive covariance kernels.\n\n\n\n\n\n","category":"type"},{"location":"gp.html#AutoGP.GP.BinaryOpNode","page":"Gaussian Process Library","title":"AutoGP.GP.BinaryOpNode","text":"abstract type BinaryOpNode <: Node end\n\nAbstract class for [composite covariance kernels](@ref (@ref gpcovkernel_comp).\n\n\n\n\n\n","category":"type"},{"location":"gp.html#AutoGP.GP.pretty","page":"Gaussian Process Library","title":"AutoGP.GP.pretty","text":"pretty(node::Node)\n\nReturn a pretty String representation of node.\n\n\n\n\n\n","category":"function"},{"location":"gp.html#Base.size","page":"Gaussian Process Library","title":"Base.size","text":"Base.size(node::Node)\nBase.size(node::LeafNode) = 1\nBase.size(a::LeafNode, b::Node) = size(a) + size(b)\n\nReturn the total number of subexpressions in a Node, as defined above.\n\n\n\n\n\n","category":"function"},{"location":"gp.html#AutoGP.GP.eval_cov","page":"Gaussian Process Library","title":"AutoGP.GP.eval_cov","text":"eval_cov(node::Node, t1::Real, t2::Real)\neval_cov(node::Node, ts::Vector{Float64})\n\nEvaluate the covariance function node at the given time indexes. The first form returns a Real number and the second form returns a covariance Matrix.\n\n\n\n\n\n","category":"function"},{"location":"gp.html#AutoGP.GP.compute_cov_matrix","page":"Gaussian Process Library","title":"AutoGP.GP.compute_cov_matrix","text":"compute_cov_matrix(node::Node, noise, ts)\n\nNon-vectorized implementation of compute_cov_matrix_vectorized.\n\n\n\n\n\n","category":"function"},{"location":"gp.html#AutoGP.GP.compute_cov_matrix_vectorized","page":"Gaussian Process Library","title":"AutoGP.GP.compute_cov_matrix_vectorized","text":"compute_cov_matrix_vectorized(node::Node, noise, ts)\n\nCompute covariance matrix by evaluating node on all pair of ts. The noise is added to the diagonal of the covariance matrix, which means that if ts[i] == ts[j], then X[ts[i]] and Xs[ts[j]] are i.i.d. samples of the true function at ts[i] plus mean zero Gaussian noise.\n\n\n\n\n\n","category":"function"},{"location":"gp.html#AutoGP.GP.extract_kernel","page":"Gaussian Process Library","title":"AutoGP.GP.extract_kernel","text":"extract_kernel(node::Node, ::Type{T}; retain::Bool=false) where T<:LeafNode\n\nRetain only those primitive kernels in node of type T <: LeafNode, by replacing all other primitive kernels with an appropriate dummy kernel:\n\nConstant(0) for Plus\nConstant(0) for ChangePoint\nConstant(1) for Plus.\n\nIf all primitive kernels in node are of type T, the return value is Constant(0).\n\nIf retain=false then the behavior is flipped: the primitive kernels of type T are removed, while the others are retained.\n\n\n\n\n\n","category":"function"},{"location":"gp.html#AutoGP.GP.split_kernel_sop","page":"Gaussian Process Library","title":"AutoGP.GP.split_kernel_sop","text":"split_kernel_sop(node::Node, ::Type{T}) where T<:LeafNode\n\nSplits the kernel k denoted by node according to a sum-of-products interpretation. In particular, write\n\nk = k_11k_12cdots k_1n_1 + k_21k_22cdots k_2n_2 + dots + k_m1k_m2cdots k_m n_m\n\nFor a given primitive base kernel type T we can rewrite the above expression as\n\nk = k^rm T + k^rm nT\n\nwhere k^rm T contains all addends with a factor of type T, and k^rm nT are the addends without a factor of type T.\n\nThe function returns a pair (node_a, node_b) corresponding to k^rm T and k^rm nT above, with Constant(0) serving as the sentinel value.\n\nExamples\n\njulia> l = Linear(1); p = Periodic(1,1); c = Constant(1)\njulia> split_kernel_sop(l, Linear)\n(l, Constant(0))\njulia> split_kernel_sop(l, Periodic)\n(Constant(0), l)\njulia> split_kernel_sop(l*p + l*c, Periodic)\n(l*p, l*c)\njulia> split_kernel_sop(p*p, Periodic)\n(p*p, Constant(0))\n\n\n\n\n\n","category":"function"},{"location":"gp.html#AutoGP.GP.reparameterize","page":"Gaussian Process Library","title":"AutoGP.GP.reparameterize","text":"reparameterize(node::Node, t::LinearTransform)\n\nReparameterize the covariance kernel according to the given LinearTransform applied to the input (known as an \"input warping\"). For a kernel k(cdotcdot theta) and a linear transform f(t) = at+b over the time domain, this function returns a kernel with new parameters theta such that k(at+b au+b theta) = k(t u theta).\n\n\n\n\n\n","category":"function"},{"location":"gp.html#AutoGP.GP.rescale","page":"Gaussian Process Library","title":"AutoGP.GP.rescale","text":"rescale(node::Node, t::LinearTransform)\n\nRescale the covariance kernel according to the given LinearTransform applied to the output. In particular, for a GP X sim mathrmGP(0 k(cdotcdot theta)) and a transformation Y = aX + b, this function returns a kernel with new parameters theta such that Y sim mathrmGP(b k(cdotcdot theta)).\n\n\n\n\n\n","category":"function"},{"location":"gp.html#gp_cov_kernel_prim","page":"Gaussian Process Library","title":"Primitive Kernels","text":"","category":"section"},{"location":"gp.html","page":"Gaussian Process Library","title":"Gaussian Process Library","text":"Notation. In this section, generic parameters (e.g., theta, theta_1, theta_2), are used to denote fieldnames of the corresponding Julia structs in the same order as they appear in the constructors.","category":"page"},{"location":"gp.html#AutoGP.GP.WhiteNoise","page":"Gaussian Process Library","title":"AutoGP.GP.WhiteNoise","text":"WhiteNoise(value)\n\nWhite noise covariance kernel.\n\nk(t t) = mathbfIt = t theta\n\nThe random variables Xt and Xt are perfectly correlated whenever t = t and independent otherwise. This kernel cannot be used to represent the joint distribution of multiple i.i.d. measurements of Xt, instead see compute_cov_matrix_vectorized.\n\n\n\n\n\n","category":"type"},{"location":"gp.html#AutoGP.GP.Constant","page":"Gaussian Process Library","title":"AutoGP.GP.Constant","text":"Constant(value)\n\nConstant covariance kernel.\n\nk(tt) = theta\n\nDraws from this kernel are horizontal lines, where theta determines the variance of the constant value around the mean (typically zero).\n\n\n\n\n\n","category":"type"},{"location":"gp.html#AutoGP.GP.Linear","page":"Gaussian Process Library","title":"AutoGP.GP.Linear","text":"Linear(intercept[, bias=1, amplitude=1])\n\nLinear covariance kernel.\n\nk(t t) = theta_2 + theta_3 (t - theta_1)(t-theta_1)\n\nDraws from this kernel are sloped lines in the 2D plane. The time intercept is theta_1. The variance around the time intercept is theta_2. The scale factor, which dictates the slope, is theta_3.\n\n\n\n\n\n","category":"type"},{"location":"gp.html#AutoGP.GP.SquaredExponential","page":"Gaussian Process Library","title":"AutoGP.GP.SquaredExponential","text":"SquaredExponential(lengthscale[, amplitude=1])\n\nSquared Exponential covariance kernel.\n\nk(tt) = theta_2 expleft(-12t-ttheta_2)^2 right)\n\nDraws from this kernel are smooth functions.\n\n\n\n\n\n","category":"type"},{"location":"gp.html#AutoGP.GP.GammaExponential","page":"Gaussian Process Library","title":"AutoGP.GP.GammaExponential","text":"GammaExponential(lengthscale, gamma[, amplitude=1])\n\nGamma Exponential covariance kernel.\n\nk(tt) = theta_3 exp(-(t-ttheta_1)^theta_2)\n\nRequires 0 < gamma <= 2. Recovers the SquaredExponential kernel when gamma = 2.\n\n\n\n\n\n","category":"type"},{"location":"gp.html#AutoGP.GP.Periodic","page":"Gaussian Process Library","title":"AutoGP.GP.Periodic","text":"Periodic(lengthscale, period[, amplitude=1])\n\nPeriodic covariance kernel.\n\nk(tt) = expleft( (-2theta_1^2) sin^2((pitheta_2) t-t) right)\n\nThe lengthscale determines how smooth the periodic function is within each period. Heuristically, the periodic kernel can be understood as:\n\nSampling X(t) t in 0p sim mathrmGP(0 mathrmSE(theta_1)).\nRepeating this fragment for all intervals jp (j+1)p j in mathbbZ.\n\n\n\n\n\n","category":"type"},{"location":"gp.html#gp_cov_kernel_comp","page":"Gaussian Process Library","title":"Composite Kernels","text":"","category":"section"},{"location":"gp.html#AutoGP.GP.Times","page":"Gaussian Process Library","title":"AutoGP.GP.Times","text":"Times(left::Node, right::Node)\nBase.:*(left::Node, right::Node)\n\nCovariance kernel obtained by multiplying two covariance kernels pointwise.\n\nk(tt) = k_rm left(tt) times k_rm right(tt)\n\n\n\n\n\n","category":"type"},{"location":"gp.html#AutoGP.GP.Plus","page":"Gaussian Process Library","title":"AutoGP.GP.Plus","text":"Plus(left::Node, right::Node)\nBase.:+(left::Node, right::Node)\n\nCovariance kernel obtained by summing two covariance kernels pointwise.\n\nk(tt) = k_rm left(tt) + k_rm right(tt)\n\n\n\n\n\n","category":"type"},{"location":"gp.html#AutoGP.GP.ChangePoint","page":"Gaussian Process Library","title":"AutoGP.GP.ChangePoint","text":"ChangePoint(left::Node, right::Node, location::Real, scale::Real)\n\nCovariance kernel obtained by switching between two kernels at location.\n\nbeginaligned\nk(tt) = sigma_1 cdot k_rm left(t t) cdot sigma_2 + (1 - sigma_1) cdot k_rm right(t t) cdot (1-sigma_2) \nmathrmwhere\nsigma_1 = (1 + tanh((t - theta_1)  theta_2))2 \nsigma_2 = (1 + tanh((t - theta_1)  theta_2))2\nendaligned\n\nThe location parameter theta_1 denotes the time point at which the change occurs.  The scale parameter theta_2 is a nonnegative number that controls the rate of change; its behavior can be understood by analyzing the two extreme values:\n\nIf location=0 then k_rm left is active and k_rm right is inactive for all times less than location; k_rm right is active and k_rm left is inactive for all times greater than location; and Xt perp Xt for all t and t on opposite sides of location.\nIf location=Inf then k_rm left and k_rm right have equal effect for all time points, and k(tt) = 12 (k_rm left(tt) + k_rm right(tt)), which is equivalent to a Plus kernel scaled by a factor of 12.\n\n\n\n\n\n","category":"type"},{"location":"gp.html#Prediction-Utilities","page":"Gaussian Process Library","title":"Prediction Utilities","text":"","category":"section"},{"location":"gp.html#Distributions.MvNormal","page":"Gaussian Process Library","title":"Distributions.MvNormal","text":"dist = Distributions.MvNormal(\n        node::Node,\n        noise::Float64,\n        ts::Vector{Float64},\n        xs::Vector{Float64},\n        ts_pred::Vector{Float64};\n        noise_pred::Union{Nothing,Float64}=nothing)\n\nReturn MvNormal posterior predictive distribution over xs_pred at time indexes ts_pred, given noisy observations [ts, xs] and covariance function node with given level of observation noise.\n\nBy default, the observation noise (noise_pred) of the new data is equal to the noise of the observed data; use noise_pred = 0. to obtain the predictive distribution over noiseless future values.\n\nSee also\n\nTo compute log probabilities, Distributions.logpdf\nTo generate samples, Base.rand\nTo compute quantiles, Distributions.quantile\n\n\n\n\n\n","category":"type"},{"location":"gp.html#Statistics.quantile","page":"Gaussian Process Library","title":"Statistics.quantile","text":"Distributions.quantile(dist::Distributions.MvNormal, p)\n\nCompute quantiles of marginal distributions of dist.\n\nExamples\n\nDistributions.quantile(Distributions.MvNormal([0,1,2,3], LinearAlgebra.I(4)), .5)\nDistributions.quantile(Distributions.MvNormal([0,1,2,3], LinearAlgebra.I(4)), [[.1, .5, .9]])\n\n\n\n\n\n","category":"function"},{"location":"gp.html#Prior-Configuration","page":"Gaussian Process Library","title":"Prior Configuration","text":"","category":"section"},{"location":"gp.html#AutoGP.GP.GPConfig","page":"Gaussian Process Library","title":"AutoGP.GP.GPConfig","text":"config = GPConfig(kwargs...)\n\nConfiguration of prior distribution over Gaussian process kernels, i.e., an instance of Node. The main kwargs (all optional) are:\n\nnode_dist_leaf::Vector{Real}: Prior distribution over LeafNode kernels; default is uniform.\nnode_dist_nocp::Vector{Real}: Prior distribution over BinaryOpNode kernels; only used if changepoints=false.\nnode_dist_cp::Vector{Real}: Prior distribution over BinaryOpNode kernels; only used if changepoints=true.\nmax_depth::Integer: Maximum depth of covariance node; default is -1 for unbounded.\nchangepoints::Bool: Whether to permit ChangePoint compositions; default is true.\nnoise::Union{Nothing,Float64}: Whether to use a fixed observation noise; default is nothing to infer automatically.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/decomposition.html#Time-Series-Decomposition","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"","category":"section"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"This tutorial shows how to decompose AutoGP models into their constituent temporal components, to gain more insight into the learned time series structures.","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"There are many ways to decompose covariance kernels. We will demonstrate three methods:","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"AutoGP.decompose. This function breaks down a composite kernel into the constituent subkernels in the expression tree.","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"AutoGP.extract_kernel. This function extracts a specific primitive kernel from a composite kernel, while discarding the others.\nAutoGP.split_kernel_sop. This function splits a kernel into a pair of kernels through a sum-of-products interpretation.","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"import AutoGP","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"using CSV\nusing Dates\nusing DataFrames\nusing PythonPlot","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"AutoGP.seed!(10)","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"data = CSV.File(\"assets/M1266.csv\")\nM3 = DataFrame(data);\ndf = M3[:,[\"ds\",\"y\"]];","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"We next split the data into a training set and test set.","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"n_test = 18\nn_train = DataFrames.nrow(df) - n_test\ndf_train = df[1:end-n_test, :]\ndf_test = df[end-n_test+1:end, :]\n\nfig, ax = PythonPlot.subplots(figsize=(10,4))\nax.scatter(df_train.ds, df_train.y, marker=\"o\", color=\"k\", alpha=.5)\nax.scatter(df_test.ds, df_test.y, marker=\"o\", color=\"w\", edgecolor=\"k\", label=\"Test Data\")","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"(Image: png)","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"Python: <matplotlib.collections.PathCollection object at 0x713940142a80>","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"model = AutoGP.GPModel(df_train.ds, df_train.y; n_particles=18);","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"ds_future = range(start=df.ds[end]+Dates.Month(1), step=Dates.Month(1), length=4*size(df_test)[1])\nds_query = vcat(df_train.ds, df_test.ds, ds_future)\nforecasts = AutoGP.predict(model, ds_query; quantiles=[0.025, 0.975]);","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"AutoGP.fit_smc!(model; schedule=vcat(collect(range(2, n_train, step=12)), n_train), n_mcmc=100, n_hmc=20, verbose=false);","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"forecasts = AutoGP.predict(model, ds_query; quantiles=[0.025, 0.975]);","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"fig, ax = PythonPlot.subplots(figsize=(10,4))\nfor i=1:AutoGP.num_particles(model)\n    subdf = forecasts[forecasts.particle.==i,:]\n    ax.plot(subdf[!,\"ds\"], subdf[!,\"y_mean\"], color=\"k\", linewidth=.5)\n    ax.fill_between(\n        subdf.ds, subdf[!,\"y_0.025\"], subdf[!,\"y_0.975\"];\n        color=\"tab:blue\", alpha=0.05)\nend\nax.scatter(df_train.ds, df_train.y, marker=\"o\", color=\"k\", label=\"Observed Data\")\nax.scatter(df_test.ds, df_test.y, marker=\"o\", color=\"w\", edgecolor=\"k\", label=\"Test Data\")","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"(Image: png)","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"Python: <matplotlib.collections.PathCollection object at 0x7138acd17d10>","category":"page"},{"location":"tutorials/decomposition.html#Hierarchical-Decomposition-of-Kernels","page":"Time Series Decomposition","title":"Hierarchical Decomposition of Kernels","text":"","category":"section"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"Let us first inspect the learned kernels.","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"weights = AutoGP.particle_weights(model)\nkernels = AutoGP.covariance_kernels(model)\nfor (i, (k, w)) in enumerate(zip(kernels, weights))\n    println(\"Model $(i), Weight $(w)\")\n    display(k)\nend","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"Model 1, Weight 0.00901154857652315\n\n\n\n×\n├── +\n│   ├── LIN(659981029.03; 97715.80, 0.00)\n│   └── LIN(1195212711.94; 560685.09, 0.00)\n└── +\n    ├── +\n    │   ├── ×\n    │   │   ├── ×\n    │   │   │   ├── ×\n    │   │   │   │   ├── LIN(645756746.98; 0.12, 0.00)\n    │   │   │   │   └── LIN(456367123.42; 0.16, 0.00)\n    │   │   │   └── LIN(567802575.74; 0.04, 0.00)\n    │   │   └── GE(74314112.16, 1.31; 0.38)\n    │   └── PER(1.74, 31698075.82; 0.14)\n    └── LIN(435409162.25; 0.10, 0.00)\n\n\n\nModel 2, Weight 0.019612029584188975\n\n\n\n×\n├── LIN(553528205.92; 193073.36, 0.00)\n└── +\n    ├── GE(610350376.93, 1.32; 0.44)\n    └── PER(1.48, 31339381.01; 0.17)\n\n\n\nModel 3, Weight 0.017007994352312553\n\n\n\n+\n├── ×\n│   ├── ×\n│   │   ├── LIN(466187204.31; 254341.57, 0.00)\n│   │   └── LIN(883086760.04; 0.22, 0.00)\n│   └── +\n│       ├── +\n│       │   ├── ×\n│       │   │   ├── GE(72266269.78, 1.63; 0.14)\n│       │   │   └── LIN(444058228.24; 0.06, 0.00)\n│       │   └── +\n│       │       ├── LIN(724687893.47; 0.08, 0.00)\n│       │       └── LIN(560627320.97; 0.75, 0.00)\n│       └── PER(1.09, 31585406.18; 0.13)\n└── LIN(558190191.81; 4095788.70, 0.00)\n\n\n\nModel 4, Weight 0.11069109354255227\n\n\n\n+\n├── +\n│   ├── GE(566952789.69, 1.66; 53807.36)\n│   └── PER(1.86, 31493709.45; 126071.89)\n└── LIN(506782708.52; 24208.61, 0.00)\n\n\n\nModel 5, Weight 0.015355520708968496\n\n\n\n+\n├── ×\n│   ├── ×\n│   │   ├── LIN(703521266.93; 402680.15, 0.00)\n│   │   └── LIN(474054605.17; 0.29, 0.00)\n│   └── +\n│       ├── ×\n│       │   ├── LIN(477789221.18; 0.08, 0.00)\n│       │   └── +\n│       │       ├── GE(93852771.12, 0.56; 0.04)\n│       │       └── LIN(445472740.93; 0.11, 0.00)\n│       └── PER(1.43, 31593348.89; 0.18)\n└── LIN(559007522.80; 27079.33, 0.00)\n\n\n\nModel 6, Weight 0.02878043770753873\n\n\n\n+\n├── ×\n│   ├── ×\n│   │   ├── LIN(455085567.29; 199889.05, 0.00)\n│   │   └── LIN(905419786.45; 0.23, 0.00)\n│   └── +\n│       ├── ×\n│       │   ├── LIN(640199044.16; 0.06, 0.00)\n│       │   └── GE(100589877.05, 1.25; 0.18)\n│       └── PER(1.43, 31593348.89; 0.18)\n└── LIN(593954212.76; 301975.24, 0.00)\n\n\n\nModel 7, Weight 0.014477329933002096\n\n\n\n+\n├── ×\n│   ├── +\n│   │   ├── LIN(497587817.58; 639968.07, 0.00)\n│   │   └── LIN(471057791.81; 1253031.98, 0.00)\n│   └── +\n│       ├── +\n│       │   ├── ×\n│       │   │   ├── ×\n│       │   │   │   ├── ×\n│       │   │   │   │   ├── LIN(458436935.16; 0.16, 0.00)\n│       │   │   │   │   └── +\n│       │   │   │   │       ├── LIN(610953981.25; 0.16, 0.00)\n│       │   │   │   │       └── ×\n│       │   │   │   │           ├── ×\n│       │   │   │   │           │   ├── LIN(478408274.62; 0.04, 0.00)\n│       │   │   │   │           │   └── +\n│       │   │   │   │           │       ├── LIN(574886530.87; 0.32, 0.00)\n│       │   │   │   │           │       └── LIN(460056017.66; 0.30, 0.00)\n│       │   │   │   │           └── ×\n│       │   │   │   │               ├── +\n│       │   │   │   │               │   ├── +\n│       │   │   │   │               │   │   ├── LIN(667199694.27; 0.39, 0.00)\n│       │   │   │   │               │   │   └── ×\n│       │   │   │   │               │   │       ├── GE(43654821.17, 1.55; 0.30)\n│       │   │   │   │               │   │       └── +\n│       │   │   │   │               │   │           ├── LIN(559025780.12; 0.03, 0.00)\n│       │   │   │   │               │   │           └── ×\n│       │   │   │   │               │   │               ├── +\n│       │   │   │   │               │   │               │   ├── PER(0.14, 80045439.57; 0.18)\n│       │   │   │   │               │   │               │   └── +\n│       │   │   │   │               │   │               │       ├── +\n│       │   │   │   │               │   │               │       │   ├── LIN(539688999.60; 0.20, 0.00)\n│       │   │   │   │               │   │               │       │   └── PER(0.19, 19029757.25; 0.04)\n│       │   │   │   │               │   │               │       └── ×\n│       │   │   │   │               │   │               │           ├── ×\n│       │   │   │   │               │   │               │           │   ├── PER(0.12, 77479085.99; 0.07)\n│       │   │   │   │               │   │               │           │   └── PER(0.18, 144355664.13; 0.01)\n│       │   │   │   │               │   │               │           └── GE(44444499.68, 0.71; 0.35)\n│       │   │   │   │               │   │               └── GE(27066775.64, 1.55; 0.13)\n│       │   │   │   │               │   └── GE(25937540.58, 1.13; 0.03)\n│       │   │   │   │               └── PER(0.29, 30436188.75; 0.13)\n│       │   │   │   └── GE(360706174.03, 1.63; 0.04)\n│       │   │   └── GE(54699037.16, 1.18; 0.23)\n│       │   └── PER(2.03, 31575590.50; 0.15)\n│       └── LIN(578615206.91; 0.97, 0.00)\n└── LIN(450947315.34; 756380.09, 0.00)\n\n\n\nModel 8, Weight 0.047447926279920595\n\n\n\n+\n├── +\n│   ├── GE(490829705.72, 1.61; 53891.92)\n│   └── PER(1.86, 31474242.59; 140799.04)\n└── +\n    ├── +\n    │   ├── LIN(719928809.88; 140065.50, 0.00)\n    │   └── LIN(450286397.01; 73853.58, 0.00)\n    └── LIN(425778979.01; 283629.75, 0.00)\n\n\n\nModel 9, Weight 0.010109195479711725\n\n\n\n×\n├── LIN(510605573.28; 538468.10, 0.00)\n└── +\n    ├── ×\n    │   ├── +\n    │   │   ├── ×\n    │   │   │   ├── ×\n    │   │   │   │   ├── GE(73592603.05, 1.32; 0.30)\n    │   │   │   │   └── GE(435853766.92, 0.97; 0.18)\n    │   │   │   └── LIN(578736277.07; 0.10, 0.00)\n    │   │   └── LIN(485369591.57; 0.50, 0.00)\n    │   └── LIN(571286497.84; 0.34, 0.00)\n    └── +\n        ├── LIN(472756425.93; 0.15, 0.00)\n        └── PER(1.29, 31481465.66; 0.16)\n\n\n\nModel 10, Weight 0.00019137991483466764\n\n\n\n×\n├── LIN(568090873.20; 139939.21, 0.00)\n└── +\n    ├── ×\n    │   ├── GE(817646585.97, 1.40; 0.50)\n    │   └── +\n    │       ├── LIN(444799222.44; 0.03, 0.00)\n    │       └── LIN(450268972.65; 0.28, 0.00)\n    └── PER(1.24, 31953635.36; 0.17)\n\n\n\nModel 11, Weight 0.0054314969855522\n\n\n\n+\n├── ×\n│   ├── ×\n│   │   ├── LIN(568363450.73; 442906.88, 0.00)\n│   │   └── LIN(478193065.57; 0.35, 0.00)\n│   └── +\n│       ├── GE(129762049.27, 1.64; 0.07)\n│       └── PER(1.42, 31677669.77; 0.25)\n└── LIN(479392279.90; 35663.69, 0.00)\n\n\n\nModel 12, Weight 0.0076299623510348\n\n\n\n+\n├── ×\n│   ├── LIN(794821416.44; 95231.27, 0.00)\n│   └── +\n│       ├── GE(212766080.39, 1.53; 0.08)\n│       └── PER(1.28, 31642882.88; 0.21)\n└── ×\n    ├── LIN(1501215005.41; 111622.20, 0.00)\n    └── LIN(551494152.52; 0.15, 0.00)\n\n\n\nModel 13, Weight 0.3270158643889261\n\n\n\n+\n├── +\n│   ├── GE(308405091.32, 1.55; 46032.60)\n│   └── PER(1.86, 31474242.59; 140799.04)\n└── LIN(478990944.11; 87109.72, 0.00)\n\n\n\nModel 14, Weight 0.08141232858111867\n\n\n\n+\n├── ×\n│   ├── LIN(468516482.12; 1517671.74, 0.00)\n│   └── +\n│       ├── ×\n│       │   ├── ×\n│       │   │   ├── ×\n│       │   │   │   ├── LIN(458383824.50; 0.08, 0.00)\n│       │   │   │   └── LIN(490260027.05; 0.05, 0.00)\n│       │   │   └── GE(490829705.72, 1.61; 0.05)\n│       │   └── GE(67306478.43, 1.19; 0.35)\n│       └── PER(1.86, 31474242.59; 0.14)\n└── LIN(611309629.23; 173952.01, 0.00)\n\n\n\nModel 15, Weight 0.0005021571370656624\n\n\n\n+\n├── +\n│   ├── ×\n│   │   ├── LIN(609809419.35; 610124.73, 0.00)\n│   │   └── +\n│   │       ├── +\n│   │       │   ├── ×\n│   │       │   │   ├── ×\n│   │       │   │   │   ├── GE(78117685.74, 1.58; 0.04)\n│   │       │   │   │   └── LIN(617136973.20; 0.35, 0.00)\n│   │       │   │   └── LIN(556041942.24; 0.27, 0.00)\n│   │       │   └── LIN(1202344796.78; 0.20, 0.00)\n│   │       └── PER(1.91, 31684257.18; 0.22)\n│   └── LIN(424034379.19; 103118.46, 0.00)\n└── LIN(468608844.61; 212589.52, 0.00)\n\n\n\nModel 16, Weight 0.2575040599217603\n\n\n\n+\n├── +\n│   ├── GE(490829705.72, 1.61; 53891.92)\n│   └── PER(1.86, 31474242.59; 140799.04)\n└── LIN(527880879.77; 307350.63, 0.00)\n\n\n\nModel 17, Weight 0.027518336753961727\n\n\n\n×\n├── LIN(468175029.06; 113767.87, 0.00)\n└── +\n    ├── +\n    │   ├── LIN(525326031.11; 0.19, 0.00)\n    │   └── GE(320816202.09, 1.73; 0.99)\n    └── +\n        ├── LIN(556245068.01; 0.17, 0.00)\n        └── PER(1.40, 31519222.30; 0.21)\n\n\n\nModel 18, Weight 0.020301337801024034\n\n\n\n+\n├── GE(361385655.65, 1.74; 365081.64)\n└── PER(1.30, 31583405.26; 131132.44)","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"We now use AutoGP.decompose to hierarchically break down the composite kernel into all the constituent subkernels.","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"decomposed_models = AutoGP.decompose(model);","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"# Helper function to pretty print covariance.\nfunction show_string(x)\n    io = IOBuffer()\n    Base.show(io, MIME(\"text/plain\"), x)\n    return String(take!(io))\nend\n\n# Plot the decomposition of `model` into its constituent parts.\nfunction plot_decomposition(model::AutoGP.GPModel)\n    kernels = AutoGP.covariance_kernels(model)\n    forecasts = AutoGP.predict(\n        model, ds_query;\n        quantiles=[0.025, 0.975]);\n    fig, axes = PythonPlot.subplots(\n        nrows=AutoGP.num_particles(model),\n        ncols=2,\n        tight_layout=true,\n        figsize=(12, 6*AutoGP.num_particles(model)),\n        )\n    for i=1:AutoGP.num_particles(model)\n        subdf = forecasts[forecasts.particle.==i,:]\n        # axes[i].set_title(show_string(kernels[i]), ha=\"left\")\n        axes[i-1,0].plot(subdf[!,\"ds\"], subdf[!,\"y_mean\"], color=\"k\", linewidth=1, label=show_string(kernels[i]))\n        axes[i-1,0].fill_between(\n            subdf.ds, subdf[!,\"y_0.025\"], subdf[!,\"y_0.975\"];\n            color=\"tab:blue\", alpha=0.05)\n        axes[i-1,0].scatter(df_train.ds, df_train.y, marker=\"o\", color=\"k\", label=\"Observed Data\")\n        axes[i-1,0].scatter(df_test.ds, df_test.y, marker=\"o\", color=\"w\", edgecolor=\"k\", label=\"Test Data\")\n        axes[i-1,1].text(0.5, 0.5, show_string(kernels[i]), transform=axes[i-1,1].transAxes,  va=\"center\", ha=\"left\")\n        axes[i-1,1].set_axis_off()\n    end\n    return fig, axes\nend","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"plot_decomposition (generic function with 1 method)","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"Let us plot the decomposition of a given particle in the ensemble.","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"idx = 13\nfig, ax = plot_decomposition(decomposed_models[11]);\nfig.suptitle(\"Decomposition of Learned Model $(idx)\", fontsize=18, va=\"center\", y=1);","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"(Image: png)","category":"page"},{"location":"tutorials/decomposition.html#STL-Style-Decomposition","page":"Time Series Decomposition","title":"STL Style Decomposition","text":"","category":"section"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"An alternative approach to decomposing kernels is using AutoGP.extract_kernel, which retains only a specific primitive kernel while discarding the others. In the following example, we will extract the AutoGP.GP.Linear, AutoGP.GP.Periodic, and AutoGP.GP.GammaExponential kernels from each learned particle to produce an \"STL\" style decomposition.","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"model_per = AutoGP.extract_kernel(model, AutoGP.GP.Periodic);\nmodel_ge = AutoGP.extract_kernel(model, AutoGP.GP.GammaExponential);\nmodel_lin = AutoGP.extract_kernel(model, AutoGP.GP.Linear);","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"Let us study the original and decomposed kernels for a given particle.","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"Unlike a traditional time series decomposition, which typically assumes a fixed additive or multiplicative structure, these decompositions retain the learned structure. For example, the decomposition for Linear may have a quadratic term, if the overall kernel has a subexpression of the form LIN * LIN.","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"The kernel structure is retained by using the AutoGP.GP.Constant to act as a \"noop\", as shown below. See also AutoGP.extract_kernel for full details.","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"idx = 2\nprintln(\"Model $(idx) - FULL\"); display(AutoGP.covariance_kernels(model)[2])\nprintln(\"Model $(idx) - LIN only\"); display(AutoGP.covariance_kernels(model_lin)[2])\nprintln(\"Model $(idx) - PER only\"); display(AutoGP.covariance_kernels(model_per)[2])\nprintln(\"Model $(idx) - GE only\"); display(AutoGP.covariance_kernels(model_ge)[2])","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"Model 2 - FULL\n\n\n\n×\n├── LIN(553528205.92; 193073.36, 0.00)\n└── +\n    ├── GE(610350376.93, 1.32; 0.44)\n    └── PER(1.48, 31339381.01; 0.17)\n\n\n\nModel 2 - LIN only\n\n\n\n×\n├── LIN(553528205.92; 193073.36, 0.00)\n└── +\n    ├── CONST(0.00)\n    └── CONST(0.00)\n\n\n\nModel 2 - PER only\n\n\n\n×\n├── CONST(985751.12)\n└── +\n    ├── CONST(0.00)\n    └── PER(1.48, 31339381.01; 0.17)\n\n\n\nModel 2 - GE only\n\n\n\n×\n├── CONST(985751.12)\n└── +\n    ├── GE(610350376.93, 1.32; 0.44)\n    └── CONST(0.00)","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"We can now obtain forecasts corresponding to the Linear, Periodic, and GammaExponential components in each particle.","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"forecasts_lin = AutoGP.predict(model_lin, ds_query .+ Day(1); quantiles=[0.025, 0.975]);\nforecasts_per = AutoGP.predict(model_per, ds_query .+ Day(1); quantiles=[0.025, 0.975]);\nforecasts_ge = AutoGP.predict(model_ge, ds_query .+ Day(1); quantiles=[0.025, 0.975]);","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"fig, axes = PythonPlot.subplots(figsize=(10,14), nrows=3, tight_layout=true)\nfor (ax, m, f) in zip(axes, [model_lin, model_per, model_ge], [forecasts_lin, forecasts_per, forecasts_ge])\n    for i=1:AutoGP.num_particles(m)\n        subdf = f[f.particle.==i,:]\n        ax.plot(subdf[!,\"ds\"], subdf[!,\"y_mean\"], color=\"k\", linewidth=.5)\n        ax.fill_between(subdf.ds, subdf[!,\"y_0.025\"], subdf[!,\"y_0.975\"]; color=\"tab:blue\", alpha=0.05)\n    end\n    ax.scatter(df_train.ds, df_train.y, marker=\"o\", color=\"k\", label=\"Observed Data\")\n    ax.scatter(df_test.ds, df_test.y, marker=\"o\", color=\"w\", edgecolor=\"k\", label=\"Test Data\")\nend\naxes[0].set_title(\"STRUCTURE: LIN\")\naxes[1].set_title(\"STRUCTURE: PER\")\naxes[2].set_title(\"STRUCTURE: GE\")","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"(Image: png)","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"Python: Text(0.5, 1.0, 'STRUCTURE: GE')","category":"page"},{"location":"tutorials/decomposition.html#Sum-of-Products-Decomposition","page":"Time Series Decomposition","title":"Sum-of-Products Decomposition","text":"","category":"section"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"An third approach to decomposing kernels is using AutoGP.split_kernel_sop, which is based on a sum-of-products decomposition of kernels.","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"In particular, we can write any composite covariance kernel k as a sum of m products, where the ith term in the sum is a product of n_i terms:","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"k = k_11k_12cdots k_1n_1 + k_21k_22cdots k_2n_2 + dots + k_m1k_m2cdots k_m n_m","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"For a given primitive base kernel, such as Periodic, we can rewrite the above expression as","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"k = k^rm PER + k^rm NOPER","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"where k^rm PER contains all addends with a Periodic factor, and k^rm NOPER are the addends without a Periodic factor.","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"See AutoGP.GP.split_kernel_sop for additional details.","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"The following function returns a pair of AutoGP.GPModel instances based on this decomposition.","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"model_a, model_b = AutoGP.split_kernel_sop(model, AutoGP.GP.Periodic);","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"Here is an example of the decomposition on the 4th particle of model.","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"idx = 4\nprintln(\"Model $(idx) - ALL\"); display(AutoGP.covariance_kernels(model)[idx])\nprintln(\"Model $(idx) - PER\"); display(AutoGP.covariance_kernels(model_a)[idx])\nprintln(\"Model $(idx) - NO PER\"); display(AutoGP.covariance_kernels(model_b)[idx])","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"Model 4 - ALL\n\n\n\n+\n├── +\n│   ├── GE(566952789.69, 1.66; 53807.36)\n│   └── PER(1.86, 31493709.45; 126071.89)\n└── LIN(506782708.52; 24208.61, 0.00)\n\n\n\nModel 4 - PER\n\n\n\nPER(1.86, 31493709.45; 126071.89)\n\n\n\nModel 4 - NO PER\n\n\n\n+\n├── GE(566952789.69, 1.66; 53807.36)\n└── LIN(506782708.52; 24208.61, 0.00)","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"forecasts_a = AutoGP.predict(model_a, ds_query .+ Day(1); quantiles=[0.025, 0.975]);\nforecasts_b = AutoGP.predict(model_b, ds_query .+ Day(1); quantiles=[0.025, 0.975]);","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"fig, axes = PythonPlot.subplots(figsize=(10,14), nrows=2, tight_layout=true)\nfor (ax, m, f) in zip(axes, [model_a, model_b], [forecasts_a, forecasts_b])\n    for i=1:AutoGP.num_particles(m)\n        subdf = f[f.particle.==i,:]\n        ax.plot(subdf[!,\"ds\"], subdf[!,\"y_mean\"], color=\"k\", linewidth=.5)\n        ax.fill_between(subdf.ds, subdf[!,\"y_0.025\"], subdf[!,\"y_0.975\"]; color=\"tab:blue\", alpha=0.05)\n    end\n    ax.scatter(df_train.ds, df_train.y, marker=\"o\", color=\"k\", label=\"Observed Data\")\n    ax.scatter(df_test.ds, df_test.y, marker=\"o\", color=\"w\", edgecolor=\"k\", label=\"Test Data\")\nend\naxes[0].set_title(\"STRUCTURE: PER\")\naxes[1].set_title(\"STRUCTURE: NO PER\")","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"(Image: png)","category":"page"},{"location":"tutorials/decomposition.html","page":"Time Series Decomposition","title":"Time Series Decomposition","text":"Python: Text(0.5, 1.0, 'STRUCTURE: NO PER')","category":"page"},{"location":"api.html#AutoGP-API","page":"AutoGP API","title":"AutoGP API","text":"","category":"section"},{"location":"api.html","page":"AutoGP API","title":"AutoGP API","text":"The purpose of this package is to automatically infer Gaussian process (GP) models of time series data, hence the name AutoGP.","category":"page"},{"location":"api.html","page":"AutoGP API","title":"AutoGP API","text":"At a high level, a Gaussian process X(t) mid t in T  is a family of random variables indexed by some set T. For time series data, the index set T = mathbbR is typically (a subset) of the real numbers. For any list of n time points t_1 dots t_n, the prior distribution of the random vector X(t_1) dots X(t_n) is a multivariate Gaussian,","category":"page"},{"location":"api.html","page":"AutoGP API","title":"AutoGP API","text":"beginaligned\n    beginbmatrix\n        X(t_1)\n        vdots \n        X(t_n)\n    endbmatrix\nsim mathrmMultivariteNormal left(\n    beginbmatrix\n        m(t_1)\n        vdots \n        m(t_n)\n    endbmatrix\n    beginbmatrix\n        k_theta(t_1 t_1)  dots  k_theta(t_1 t_n) \n        vdots  ddots  vdots \n        k_theta(t_n t_1)  dots  k_theta(t_n t_n) \n    endbmatrix\n    right)\nendaligned","category":"page"},{"location":"api.html","page":"AutoGP API","title":"AutoGP API","text":"In the equation above m  T to mathbbR is the mean function and k_theta T times T to mathbbR_ge 0 is the covariance (kernel) function parameterized by theta. We typically assume (without loss of generality) that m(t) = 0 for all t and focus our modeling efforts on the covariance kernel k_theta. The structure of the kernel dictates the qualitative properties of X, such as the existence of linear trends, seasonal components, changepoints, etc.; refer to the kernel cookbook for an overview of these concepts.","category":"page"},{"location":"api.html","page":"AutoGP API","title":"AutoGP API","text":"Gaussian processes can be used as priors in Bayesian nonparametric models of time series data Y(t_1) dots Y(t_n) as follows. We assume that each Y(t_i) = g(X(t_i) epsilon(t_i)) for some known function g and noise process epsilon(t_i). A common choice is to let Y be a copy of X corrupted with i.i.d. Gaussian innovations, which gives Y(t_i) = X(t_i) + epsilon(t_i) where epsilon(t_i) sim mathrmNormal(0 eta) for some variance eta  0.","category":"page"},{"location":"api.html","page":"AutoGP API","title":"AutoGP API","text":"Writing out the full model","category":"page"},{"location":"api.html","page":"AutoGP API","title":"AutoGP API","text":"beginaligned\nk\n    sim mathrmPCFG\n    textrm(prior over covariance kernel) \ntheta_1 dots theta_d(k)\n    sim p_k\n     textrm(parameter prior)\neta\n    sim p_eta\n    textrm(noise prior) \nX(t_1) dots X(t_n)\n    sim mathrmMultivariteNormal(mathbf0 k_theta(t_it_j)_ij=1^n)\n    textrm(Gaussian process) \nY(t_i)\n    sim mathrmNormal(X(t_i) eta) i=1dotsn\n    textrm(noisy observations)\nendaligned","category":"page"},{"location":"api.html","page":"AutoGP API","title":"AutoGP API","text":"where PCFG denotes a probabilistic-context free grammar that defines a language of covariance kernel expressions k,","category":"page"},{"location":"api.html","page":"AutoGP API","title":"AutoGP API","text":"beginaligned\nB      = textttLinear mid textttPeriodic mid textttGammaExponential mid dots \noplus = texttt+ mid texttt* mid textttChangePoint \nk      = B mid texttt(k_1 oplus k_2texttt)\nendaligned","category":"page"},{"location":"api.html","page":"AutoGP API","title":"AutoGP API","text":"and d(k) is the number of parameters in expression k. Given data (t_iy_i)_i=1^n, AutoGP infers likely values of the symbolic structure of the covariance kernel k, the kernel parameters theta, and the observation noise variance eta.","category":"page"},{"location":"api.html","page":"AutoGP API","title":"AutoGP API","text":"The ability to automatically synthesize covariance kernels in AutoGP is in contrast to existing Gaussian process libraries such as GaussianProcess.jl sklearn.gaussian_process, GPy, and GPyTorch, which all require users to manually specify k.","category":"page"},{"location":"api.html","page":"AutoGP API","title":"AutoGP API","text":"After model fitting, users can query the models to generate forecasts, compute probabilities, and inspect qualitative structure.","category":"page"},{"location":"api.html#AutoGP","page":"AutoGP API","title":"AutoGP","text":"Main module.\n\nExports\n\n\n\n\n\n","category":"module"},{"location":"api.html#Model-Initialization","page":"AutoGP API","title":"Model Initialization","text":"","category":"section"},{"location":"api.html#AutoGP.GPModel","page":"AutoGP API","title":"AutoGP.GPModel","text":"struct GPModel\n\nA GPModel contains covariance kernel structures and parameters for modeling data.\n\nFields\n\npf_state::Gen.ParticleFilterState: Internal particle set.\nconfig::GP.GPConfig=GP.GPConfig(): User-specific customization, refer to GP.GPConfig.\nds::IndexType: Observed time points.\ny::Vector{<:Real}: Observed time series values.\nds_transform::Transforms.LinearTransform: Transformation of time to direct space.\ny_transform::Transforms.LinearTransform: Transformation of observations to direct space.\n\nConstructors\n\nmodel = GPModel(\n    ds::IndexType,\n    y::Vector{<:Real};\n    n_particles::Integer=8,\n    config::GP.GPConfig=GP.GPConfig())\n\nSee also\n\nTo perform learning given the data, refer to\n\nAutoGP.fit_smc!\nAutoGP.fit_mcmc!\nAutoGP.fit_greedy!\n\n\n\n\n\n","category":"type"},{"location":"api.html#AutoGP.num_particles","page":"AutoGP API","title":"AutoGP.num_particles","text":"num_particles(model::GPModel)\n\nReturn the number of particles.\n\n\n\n\n\n","category":"function"},{"location":"api.html#AutoGP.seed!","page":"AutoGP API","title":"AutoGP.seed!","text":"seed!(seed)\n\nSet the random seed of the global random number generator.\n\n\n\n\n\n","category":"function"},{"location":"api.html#AutoGP.IndexType","page":"AutoGP API","title":"AutoGP.IndexType","text":"IndexType = Union{Vector{<:Real}, Vector{<:Date}, Vector{<:DateTime}}\n\nPermitted Julia types for Gaussian process time points. Real numbers are ingested directly, treated as time points. Instances of the Dates types are converted to numeric time points by using Dates.datetime2unix.\n\n\n\n\n\n","category":"type"},{"location":"api.html#end_to_end_model_fitting","page":"AutoGP API","title":"End-to-End Model Fitting","text":"","category":"section"},{"location":"api.html#AutoGP.fit_smc!","page":"AutoGP API","title":"AutoGP.fit_smc!","text":"fit_smc!(\n    model::GPModel;\n    schedule::Vector{<:Integer},\n    n_mcmc::Int,\n    n_hmc::Int,\n    shuffle::Bool=true,\n    biased::Bool=false,\n    adaptive_resampling::Bool=true,\n    adaptive_rejuvenation::Bool=false,\n    hmc_config::Dict=Dict(),\n    verbose::Bool=false,\n    check::Bool=false,\n    callback_fn::Function=(; kwargs...) -> nothing)\n\nInfer the structure and parameters of an appropriate Gaussian process covariance kernel for modeling the observed data. Inference is performed using sequential Monte Carlo.\n\nArguments\n\nmodel::GPModel: Instance of the GPModel to use.\nschedule::Vector{<:Integer}: Schedule for incorporating data for SMC, refer to Schedule.\nn_mcmc::Union{Integer,Vector{<:Integer}}: Number of involutive MCMC rejuvenation steps. If vector, must have same length as schedule.\nn_hmc::Union{Integer,Vector{<:Integer}}: Number of HMC steps per accepted involutive MCMC step. If vector, must have same length as schedule.\nbiased::Bool:   Whether to bias the proposal to produce \"short\" structures.\nshuffle::Bool=true: Whether to shuffle indexes ds or incorporate data in the given order.\nadaptive_resampling::Bool=true: If true resamples based on ESS threshold, else at each step.\nadaptive_rejuvenation::Bool=false: If true rejuvenates only if resampled, else at each step.\nhmc_config::Dict: Configuration for HMC inference on numeric parameters. Allowable keys are:\nn_exit::Integer=1: Number of successive rejections after which HMC loop is terminated.\nL_param::Integer=10 Number of leapfrog steps for kernel parameters.\nL_noise::Integer=10: Number of leapfrog steps for noise parameter.\neps_param::Float64=0.02: Step size for kernel parameters.\neps_noise::Float64=0.02: Step size for noise parameter.\nverbose::Bool=false: Report progress to stdout.\ncheck::Bool=false: Perform dynamic correctness checks during inference.\nconfig::GP.GPConfig=GP.GPConfig(): User-specific customization, refer to GP.GPConfig.\ncallback_fn: A callback for monitoring inference, must be generated by AutoGP.Callbacks.make_smc_callback.\n\n\n\n\n\n","category":"function"},{"location":"api.html#AutoGP.fit_mcmc!","page":"AutoGP API","title":"AutoGP.fit_mcmc!","text":"fit_mcmc!(\n    model::GPModel;\n    n_mcmc::Integer,\n    n_hmc::Integer,\n    biased::Bool=false,\n    verbose::Bool=false,\n    check::Bool=false,\n    callback_fn::Function=(; kwargs...) -> nothing)\n\nPerform n_mcmc steps of involutive MCMC on the structure, with n_hmc steps of Hamiltonian Monte Carlo sampling on the parameters per accepted involutive MCMC move.\n\nA callback_fn can be provided to monitor the progress each MCMC step for which at least one particle (i.e, chain) accepted a transition. Its signature must contain a single varargs specifier, which will be populated with keys :model, :step, :elapsed.\n\nwarning: Warning\nThe callback_fn imposes a roughly 2x runtime overhead as compared to the equivalent mcmc_structure! method, because parallel execution must be synchronized across the particles to invoke the callback at each step. The :elapsed variable provided to the callback function will still reflect an accurate estimate of the inference runtime without this overhead. If no callback is required, use mcmc_structure! instead.\n\n\n\n\n\n","category":"function"},{"location":"api.html#AutoGP.fit_greedy!","page":"AutoGP API","title":"AutoGP.fit_greedy!","text":"function fit_greedy!(\n        model::GPModel;\n        max_depth::Integer=model.config.max_depth,\n        verbose::Bool=false,\n        check::Bool=false,\n        callback_fn::Function=(; kwargs...) -> nothing)\n\nInfer the structure and parameters of an appropriate Gaussian process covariance kernel for modeling the observed data. Inference is performed using greedy search, as described in Algorithm 2 of Kim and Teh, 2018. It is an error if max_depth is not a finite positive number.\n\nA callback_fn can be provided to monitor the search progress at each stage. Its signature must contain a single varargs specifier, which will be populated with keys :model, :step, :elapsed at each step of the greedy search.\n\n\n\n\n\n","category":"function"},{"location":"api.html#Incremental-Model-Fitting","page":"AutoGP API","title":"Incremental Model Fitting","text":"","category":"section"},{"location":"api.html#AutoGP.add_data!","page":"AutoGP API","title":"AutoGP.add_data!","text":"add_data!(model::GPModel, ds::IndexType, y::Vector{<:Real})\n\nIncorporate new observations (ds, y) into model.\n\n\n\n\n\n","category":"function"},{"location":"api.html#AutoGP.remove_data!","page":"AutoGP API","title":"AutoGP.remove_data!","text":"remove_data!(model::GPModel, ds::IndexType, y::Vector{<:Real})\n\nRemove existing observations ds from model.\n\n\n\n\n\n","category":"function"},{"location":"api.html#AutoGP.maybe_resample!","page":"AutoGP API","title":"AutoGP.maybe_resample!","text":"maybe_resample!(model::GPModel, ess_threshold::Real)\n\nResample the particle collection in model if ESS is below ess_threshold. Setting ess_threshold = AutoGP.num_particles(model) + 1 will ensure that resampling always takes place, since the ESS is upper bounded by the number of particles.\n\n\n\n\n\n","category":"function"},{"location":"api.html#AutoGP.mcmc_structure!","page":"AutoGP API","title":"AutoGP.mcmc_structure!","text":"mcmc_structure!(model::GPModel, n_mcmc::Integer, n_hmc::Integer;\n    biased::Bool=false, verbose::Bool=false, check::Bool=false)\n\nPerform n_mcmc steps of involutive MCMC on the structure, with n_hmc steps of Hamiltonian Monte Carlo sampling on the parameters per accepted involutive MCMC move.\n\n\n\n\n\n","category":"function"},{"location":"api.html#AutoGP.mcmc_parameters!","page":"AutoGP API","title":"AutoGP.mcmc_parameters!","text":"mcmc_parameters!(model::GPModel, n_hmc::Integer; verbose::Bool=false, check::Bool=false)\n\nPerform n_hmc steps of Hamiltonian Monte Carlo sampling on the parameters.\n\n\n\n\n\n","category":"function"},{"location":"api.html#model_querying","page":"AutoGP API","title":"Model Querying","text":"","category":"section"},{"location":"api.html#AutoGP.predict","page":"AutoGP API","title":"AutoGP.predict","text":"predictions = predict(\n    model::GPModel,\n    ds::IndexType;\n    quantiles::Vector{Float64}=Float64[],\n    noise_pred::Union{Nothing,Float64}=nothing)\n\nReturn predictions for new index point ds, and optionally quantiles corresponding to the provided quantiles (numbers between 0 and 1, inclusive). By default, the noise_pred of the new data is equal to the inferred noise of the observed data within each particle in model; using noise_pred=0. returns the posterior distribution over the noiseless function values.\n\nThe returned DataFrames.DataFrame has columns [\"ds\", \"particle\", \"weight\", \"y_mean\"], as well as any additional columns for the requested quantiles.\n\nExample\n\njulia> ds = [Dates.Date(2020,1,1), Dates.Date(2020,1,2)] # Dates to query\njulia> GPModel.predict(model, ds; quantiles=[.025, 0.975])\n16×6 DataFrame\n Row │ ds          particle  weight       y_0.025    y_0.975    y_mean\n     │ Date        Int64     Float64      Float64    Float64    Float64\n─────┼────────────────────────────────────────────────────────────────────\n   1 │ 2020-01-01         1  4.97761e-22  -13510.0   14070.6      280.299\n   2 │ 2020-01-02         1  4.97761e-22  -13511.0   14071.6      280.299\n   3 │ 2020-01-01         2  0.279887       4504.73   8211.43    6358.08\n   4 │ 2020-01-02         2  0.279887       4448.06   8154.3     6301.18\n   5 │ 2020-01-01         3  0.0748059    -43638.6   65083.0    10722.2\n   6 │ 2020-01-02         3  0.0748059    -43662.0   65074.7    10706.4\n   7 │ 2020-01-01         4  0.60809      -17582.2   30762.4     6590.06\n   8 │ 2020-01-02         4  0.60809      -17588.0   30771.5     6591.78\n\n\n\n\n\n","category":"function"},{"location":"api.html#AutoGP.predict_proba","page":"AutoGP API","title":"AutoGP.predict_proba","text":"function predict_proba(model::GPModel, ds::IndexType, y::Vector{<:Real})\n\nCompute predictive probability of data y at time points ds under model.\n\nExample\n\njulia> ds = [Dates.Date(2020,1,1), Dates.Date(2020,1,2)] # Dates to query\njulia> y = [0.1, .0.5] # values to query\njulia> GPModel.predict(model, ds, y)\n7×3 DataFrame\n Row │ particle  weight       logp\n     │ Int64     Float64      Float64\n─────┼─────────────────────────────────\n   1 │        1  0.0287155    -64.2388\n   2 │        2  0.0437349    -59.7672\n   3 │        3  0.576247     -62.6499\n   4 │        4  0.00164846   -59.5311\n   5 │        5  0.215255     -61.066\n   6 │        6  0.134198     -64.4041\n   7 │        7  0.000201078  -68.462\n\n\n\n\n\n","category":"function"},{"location":"api.html#AutoGP.predict_mvn","page":"AutoGP API","title":"AutoGP.predict_mvn","text":"dist = predict_mvn(model::GPModel, ds::IndexType; noise_pred::Union{Nothing,Float64}=nothing)\n\nReturn an instance of Distributions.MixtureModel representing the overall posterior predictive distribution for data at index points ds. By default, the noise_pred of the new data is equal to the inferred noise of the observed data within each particle in model; using noise_pred=0. returns the posterior distribution over the noiseless function values.\n\nThe returned dist has precisely num_particles(model) components, each of type Distributions.MvNormal, with weights particle_weights(model). These objects can be retrieved using Distributions.components and Distributions.probs, respectively.\n\n\n\n\n\n","category":"function"},{"location":"api.html#AutoGP.predict_quantile","page":"AutoGP API","title":"AutoGP.predict_quantile","text":"(x::Vector, success::Bool) = predict_quantile(\n    model::GPModel, ds::IndexType, q::Real;\n    noise_pred::Union{Nothing,Float64}=nothing, tol=1e-5, max_iter=1e6)\n\nEvaluates the inverse cumulative distribution function (CDF) of the multivariate Gaussian mixture model returned by predict_mvn at q (between 0 and 1, exclusive) separately for each dimension. The returned vector x has the same length as the index points ds.\n\nNote\n\nThe inverse CDF is numerically estimated using a binary search algorithm. The keyword arguments tol and max_iter  correspond to the desired absolute tolerance of the estimate and the maximum number of binary search iterations, respectively. The returned Boolean variable success indicates whether the returned value x has been located to within the specified error tolerance.\n\nSee also\n\npredict_mvn\n\n\n\n\n\n","category":"function"},{"location":"api.html#AutoGP.log_marginal_likelihood_estimate","page":"AutoGP API","title":"AutoGP.log_marginal_likelihood_estimate","text":"log_marginal_likelihood_estimate(model::GPModel)\n\nReturn estimate of marginal likelihood of data (in log space).\n\n\n\n\n\n","category":"function"},{"location":"api.html#AutoGP.particle_weights","page":"AutoGP API","title":"AutoGP.particle_weights","text":"particle_weights(model::GPModel)\n\nReturn vector of normalized particle weights.\n\n\n\n\n\n","category":"function"},{"location":"api.html#AutoGP.effective_sample_size","page":"AutoGP API","title":"AutoGP.effective_sample_size","text":"effective_sample_size(model::GPModel)\n\nReturn effective sample size (ESS) of weighted particle collection.\n\n\n\n\n\n","category":"function"},{"location":"api.html#AutoGP.covariance_kernels","page":"AutoGP API","title":"AutoGP.covariance_kernels","text":"covariance_kernels(model::GPModel; reparameterize::Bool=true)\n\nReturn Gaussian process covariance kernels in model. If reparameterize is true (default), then the kernel parameters are given in the original data space (more interpretable); otherwise they are given in the transformed space over which parameter inference is performed (useful for debugging).\n\n\n\n\n\n","category":"function"},{"location":"api.html#AutoGP.observation_noise_variances","page":"AutoGP API","title":"AutoGP.observation_noise_variances","text":"observation_noise_variances(model::GPModel; reparameterize::Bool=true)\n\nReturn list of observation noise variances for each particle in model. If reparameterize is true (default), then the kernel parameters are given in the original data space (more interpretable); otherwise they are given in the transformed space over which parameter inference is performed (useful for debugging).\n\n\n\n\n\n","category":"function"},{"location":"api.html#AutoGP.decompose","page":"AutoGP API","title":"AutoGP.decompose","text":"function decompose(model::GPModel)\n\nDecompose each particle within model into its constituent kernels. Supposing that num_particles(model) equals k, the return value models::Vector{GPModel} of decompose is a length-k vector of GPModel instances.\n\nTherefore, models[i] is a GPModel that represents the decomposition of particle i in model into its constituent kernels. Each particle in models[i] corresponds to a kernel fragment in the covariance for particle i of model (i.e., one particle for each GP.Node in the covariance kernel).\n\nThe weights of models[i] are arbitrary and have no meaningful value.\n\nThis function is particularly useful for visualizing the individual time series structures that make up each particle of model.\n\nSee also\n\nTime Series Decomposition Tutorial\n\n\n\n\n\n","category":"function"},{"location":"api.html#AutoGP.extract_kernel","page":"AutoGP API","title":"AutoGP.extract_kernel","text":"extract_kernel(model::GPModel, ::Type{T}; retain::Bool=true) where T <: GP.LeafNode\n\nRetain all primitive kernels of type T <:AutoGP.GP.LeafNode from each particle, while erasing the others. In particular, the erasure rules are as follows, where G != T:\n\nAutoGP.GP.Plus: k1::T + k2::G becomes k1 + Constant(0).\nAutoGP.GP.Times: k1::T * k2::G becomes k1 * Constant(1).\nAutoGP.GP.ChangePoint: CP(k1::T, k2::G) becomes CP(k1, Constant(0)).\n\nIf all the primitive kernels have type T, then the returned kernel is GP.Constant(0).\n\nIf retain=false, then the above behavior is flipped: each primitive kernel of type T is erased while all other primitive kernels are retained.\n\nSee also\n\nAutoGP.GP.extract_kernel\nTime Series Decomposition Tutorial\n\n\n\n\n\n","category":"function"},{"location":"api.html#AutoGP.split_kernel_sop","page":"AutoGP API","title":"AutoGP.split_kernel_sop","text":"split_kernel_sop(model::GPModel, ::Type{T}) where {T <: GP.LeafNode}\n\nDecompose the kernels in model through a sum-of-products interpretation. Each kernel k in model is split as k1 + k2, where k1 contains all the factors with subkernel of type T and the rest of the terms are in k2. The sentinel addend of Constant(0) is used whenever k1 or k2 is empty.\n\nThe return value is a tuple (model_a::GPModel, model_b::GPModel) where model_a contains all the k1 kernels and model_b contains all the k2 kernels.\n\nSee also\n\nAutoGP.GP.split_kernel_sop\nTime Series Decomposition Tutorial\n\n\n\n\n\n","category":"function"},{"location":"api.html#model_serialization","page":"AutoGP API","title":"Serialization","text":"","category":"section"},{"location":"api.html#Base.Dict-Tuple{AutoGP.GPModel}","page":"AutoGP API","title":"Base.Dict","text":"Base.Dict(model::GPModel)\n\nConvert a GPModel into a dictionary that can be saved and loaded from disk, as shown in the following example.\n\nExample\n\nusing AutoGP, Dates, Serialization\nmodel = AutoGP.GPModel([Date(\"2025-01-01\"), Date(\"2025-01-02\")], [1.0, 2.0])\nserialize(\"model.autogp\", Dict(model))\nloaded_model = AutoGP.GPModel(deserialize(\"model.autogp\"))\n\nwarning: Warning\nUsers should not directly serialize GPModel instances, e.g.,serialize(\"model.autogp\", model)\nmodel = deserialize(\"model.autogp\")The first line will throw ArgumentError.\n\n\n\n\n\n","category":"method"},{"location":"tutorials.html#Tutorials","page":"Tutorials","title":"Tutorials","text":"","category":"section"},{"location":"tutorials.html","page":"Tutorials","title":"Tutorials","text":"Pages = [\n    \"tutorials/overview.md\",\n    \"tutorials/iclaims.md\",\n    \"tutorials/callbacks.md\",\n    \"tutorials/greedy_mcmc.md\",\n]","category":"page"},{"location":"tutorials.html","page":"Tutorials","title":"Tutorials","text":"To run the tutorials interactively, first install IJulia and PyPlot. Then run the following commands from the terminal, making sure to replace </path/to/>AutoGP.jl with the actual path to the directory:","category":"page"},{"location":"tutorials.html","page":"Tutorials","title":"Tutorials","text":"$ export JULIA_NUM_THREADS=$(nproc)\n$ export JULIA_PROJECT=</path/to/>AutoGP.jl\n$ cd ${JULIA_PROJECT}/docs/src/tutorials\n$ julia -e 'using IJulia; notebook(dir=\".\")'","category":"page"},{"location":"tutorials.html","page":"Tutorials","title":"Tutorials","text":"The notebook server will be available in the browser at https://localhost:8888.","category":"page"},{"location":"tutorials/greedy_mcmc.html#Greedy-Search-and-MCMC","page":"Greedy Search and MCMC","title":"Greedy Search and MCMC","text":"","category":"section"},{"location":"tutorials/greedy_mcmc.html","page":"Greedy Search and MCMC","title":"Greedy Search and MCMC","text":"The Model Fitting via SMC section of the Overview tutorial showed how to learn the structure of Gaussian process time series models by using sequential Monte Carlo structure learning.  This tutorial will illustrate two alternative structure discovery methods:","category":"page"},{"location":"tutorials/greedy_mcmc.html","page":"Greedy Search and MCMC","title":"Greedy Search and MCMC","text":"Greedy search (AutoGP.fit_greedy!)\nMCMC sampling (AutoGP.fit_mcmc!)","category":"page"},{"location":"tutorials/greedy_mcmc.html","page":"Greedy Search and MCMC","title":"Greedy Search and MCMC","text":"import AutoGP\nimport CSV\nimport Dates\nimport DataFrames\nimport Distributions\n\nusing AutoGP.GP\nusing PyPlot: plt","category":"page"},{"location":"tutorials/greedy_mcmc.html","page":"Greedy Search and MCMC","title":"Greedy Search and MCMC","text":"The synthetic data generated below is the same as the data used in the Inspecting Online Inference tutorial.","category":"page"},{"location":"tutorials/greedy_mcmc.html","page":"Greedy Search and MCMC","title":"Greedy Search and MCMC","text":"AutoGP.seed!(2)\ncov = (Linear(1,0,50) * Periodic(5,2)) + GammaExponential(1,1)\nnoise = .1\nn = 100\n\nds = Vector{Float64}(range(0, 20, length=n))\ndist = Distributions.MvNormal(cov, noise, Float64[], Float64[], ds)\ny = Distributions.rand(dist);\n\nfig, ax = plt.subplots(figsize=(6, 4), dpi=100, tight_layout=true)\nax.plot(ds, y, marker=\".\", markerfacecolor=\"none\", markeredgecolor=\"k\", color=\"black\");","category":"page"},{"location":"tutorials/greedy_mcmc.html","page":"Greedy Search and MCMC","title":"Greedy Search and MCMC","text":"(Image: png)","category":"page"},{"location":"tutorials/greedy_mcmc.html#Greedy-Search","page":"Greedy Search and MCMC","title":"Greedy Search","text":"","category":"section"},{"location":"tutorials/greedy_mcmc.html","page":"Greedy Search and MCMC","title":"Greedy Search and MCMC","text":"To perform greedy search, the model::GPModel object must have a single particle.  Moreover, greedy search does not currently supported AutoGP.GP.ChangePoint covariance structures.  We can disable changepoints by using a custom AutoGP.GP.GPConfig, as shown below.","category":"page"},{"location":"tutorials/greedy_mcmc.html","page":"Greedy Search and MCMC","title":"Greedy Search and MCMC","text":"model = AutoGP.GPModel(ds, y; n_particles=1, config=GP.GPConfig(changepoints=false));","category":"page"},{"location":"tutorials/greedy_mcmc.html","page":"Greedy Search and MCMC","title":"Greedy Search and MCMC","text":"Greedy search also supports callback functions to inspect inference.  The callback function below will be invoked at each stage of the greedy search.  We will plot AIC at each stage and the corresponding covariance structure.","category":"page"},{"location":"tutorials/greedy_mcmc.html","page":"Greedy Search and MCMC","title":"Greedy Search and MCMC","text":"fig, axs = plt.subplots(nrows=2, ncols=5, figsize=(15, 15), dpi=150)\naxs = permutedims(axs)\nfig.set_tight_layout(true)\nplt.close(fig)\n\nfunction fn_callback(; kwargs...)\n    model = kwargs[:model]\n    step = kwargs[:step]\n    aic = kwargs[:aic]\n    \n    ds_query = vcat(model.ds, (20:.1:30))\n    predictions = AutoGP.predict(model, ds_query; quantiles=[0.025, 0.975])\n    \n    axs[step].scatter(model.ds, model.y, marker=\"o\", color=\"k\", s=10, label=\"Observed Data\")    \n    axs[step].axvline(model.ds[end], color=\"red\", linestyle=\"--\")\n    axs[step].plot(predictions[!,:ds], predictions[!,:y_mean], linewidth=1, color=\"k\")\n    axs[step].fill_between(\n        predictions[!,:ds],\n        predictions[!,\"y_0.025\"],\n        predictions[!,\"y_0.975\"],\n        color=\"tab:green\",\n        alpha=.25)\n\n    io = IOBuffer()\n    cov = AutoGP.covariance_kernels(model)[1]\n    Base.show(io, MIME(\"text/plain\"), cov)\n    cov_str = String(take!(io))\n    axs[step].set_title(\"Step $(step)\\nAIC:$(aic)\\n$(cov_str)\", ha=\"left\")\nend","category":"page"},{"location":"tutorials/greedy_mcmc.html","page":"Greedy Search and MCMC","title":"Greedy Search and MCMC","text":"fn_callback (generic function with 1 method)","category":"page"},{"location":"tutorials/greedy_mcmc.html","page":"Greedy Search and MCMC","title":"Greedy Search and MCMC","text":"The plots below show a key shortcoming of greedy search.  Whereas steps 3–8 contain a sensible covariance structure, at steps 9–10 the structure becomes worse.  This behavior is a result of greedy search selecting covariance structures with the smallest AIC, which is a rough heuristic to an ideal scoring function based on the marginal likelihood of the data under each structure, integrating out the parameter.  Since AIC is based on maximum likelihood estimation and the likelihood of parameters is highly multimodal for structures such as AutoGP.GP.Periodic, greedy search may result in brittle and unpredictable behavior.","category":"page"},{"location":"tutorials/greedy_mcmc.html","page":"Greedy Search and MCMC","title":"Greedy Search and MCMC","text":"AutoGP.fit_greedy!(model; max_depth=10, callback_fn=fn_callback);\nfig","category":"page"},{"location":"tutorials/greedy_mcmc.html","page":"Greedy Search and MCMC","title":"Greedy Search and MCMC","text":"(Image: png)","category":"page"},{"location":"tutorials/greedy_mcmc.html","page":"Greedy Search and MCMC","title":"Greedy Search and MCMC","text":"sys:1: UserWarning: Glyph 65291 (\\N{FULLWIDTH PLUS SIGN}) missing from current font.","category":"page"},{"location":"tutorials/greedy_mcmc.html#MCMC","page":"Greedy Search and MCMC","title":"MCMC","text":"","category":"section"},{"location":"tutorials/greedy_mcmc.html","page":"Greedy Search and MCMC","title":"Greedy Search and MCMC","text":"Markov chain Monte Carlo sampling is another structure learning method. Let us initialize a GPModel with 2 particles.","category":"page"},{"location":"tutorials/greedy_mcmc.html","page":"Greedy Search and MCMC","title":"Greedy Search and MCMC","text":"model = AutoGP.GPModel(ds, y; n_particles=2);","category":"page"},{"location":"tutorials/greedy_mcmc.html","page":"Greedy Search and MCMC","title":"Greedy Search and MCMC","text":"figures = []\nfunction fn_callback(; kwargs...)\n    model = kwargs[:model]\n    step = kwargs[:step]\n    elapsed = kwargs[:elapsed]\n\n    ds_query = vcat(model.ds, (20:.1:100))\n    predictions = AutoGP.predict(model, ds_query; quantiles=[0.025, 0.975])\n    \n    fig, axis = plt.subplots(ncols=2, figsize=(18, 6), dpi=200)\n    for (i, ax) in enumerate(axis)\n        subdf = predictions[predictions.particle.==i,:]\n        ax.scatter(model.ds, model.y, marker=\"o\", color=\"k\", s=10, label=\"Observed Data\")    \n        ax.axvline(model.ds[end], color=\"red\", linestyle=\"--\")\n        ax.plot(subdf[!,:ds], subdf[!,:y_mean], linewidth=1, color=\"k\")\n        ax.fill_between(\n            subdf[!,:ds],\n            subdf[!,\"y_0.025\"],\n            subdf[!,\"y_0.975\"],\n            color=\"tab:green\",\n            alpha=.25)\n\n        io = IOBuffer()\n        cov = AutoGP.covariance_kernels(model)[i]\n        Base.show(io, MIME(\"text/plain\"), cov)\n        cov_str = String(take!(io))\n        ax.set_title(\"Step $(step)\\nElapsed $(elapsed[i])\\n$(cov_str)\", ha=\"left\")\n    end\n    push!(figures, fig)\n    plt.close(fig)\nend","category":"page"},{"location":"tutorials/greedy_mcmc.html","page":"Greedy Search and MCMC","title":"Greedy Search and MCMC","text":"fn_callback (generic function with 1 method)","category":"page"},{"location":"tutorials/greedy_mcmc.html","page":"Greedy Search and MCMC","title":"Greedy Search and MCMC","text":"AutoGP.fit_mcmc!(model; n_mcmc=45, n_hmc=10, callback_fn=fn_callback)","category":"page"},{"location":"tutorials/greedy_mcmc.html","page":"Greedy Search and MCMC","title":"Greedy Search and MCMC","text":"Let's now show some plots that were collected by the callback during MCMC inference. The final plot shows an interesting example of how MCMC learning can reflect posterior uncertainty over the covariance structure.","category":"page"},{"location":"tutorials/greedy_mcmc.html","page":"Greedy Search and MCMC","title":"Greedy Search and MCMC","text":"display(figures[1])\ndisplay(figures[10])\ndisplay(figures[end])","category":"page"},{"location":"tutorials/greedy_mcmc.html","page":"Greedy Search and MCMC","title":"Greedy Search and MCMC","text":"(Image: png)","category":"page"},{"location":"tutorials/greedy_mcmc.html","page":"Greedy Search and MCMC","title":"Greedy Search and MCMC","text":"(Image: png)","category":"page"},{"location":"tutorials/greedy_mcmc.html","page":"Greedy Search and MCMC","title":"Greedy Search and MCMC","text":"(Image: png)","category":"page"},{"location":"tutorials/greedy_mcmc.html#MCMC-vs-SMC","page":"Greedy Search and MCMC","title":"MCMC vs SMC","text":"","category":"section"},{"location":"tutorials/greedy_mcmc.html","page":"Greedy Search and MCMC","title":"Greedy Search and MCMC","text":"MCMC sampling using AutoGP.fit_mcmc! invokes the same transition kernels over structure and parameters as those used in particle rejuvenation step of AutoGP.fit_smc!.  The main difference is that AutoGP.fit_smc! anneals the posterior over subsets of data at each step, whereas AutoGP.fit_mcmc! uses the full data at each step.","category":"page"},{"location":"tutorials/greedy_mcmc.html","page":"Greedy Search and MCMC","title":"Greedy Search and MCMC","text":"The benefits of SMC include","category":"page"},{"location":"tutorials/greedy_mcmc.html","page":"Greedy Search and MCMC","title":"Greedy Search and MCMC","text":"Runtime efficiency in cases where the structure can be quickly inferred using a subset of the data; whereas MCMC always conditions on the full data at each step yielding more expensive likelihood evaluations.\nPrincipled online inference, by using AutoGP.add_data!; whereas MCMC is an offline method.\nParticle resampling, by using AutoGP.maybe_resample!, to redirect computational effort to more promising structures and parameters; whereas MCMC iterates independent particles (i.e., chains).\nThe availability of unbiased marginal likelihood estimates (via AutoGP.log_marginal_likelihood_estimate; whereas the marginal likelihood estimate obtained from MCMC is essentially importance sampling the posterior using the prior as a proposal.","category":"page"},{"location":"tutorials/iclaims.html#Insurance-Claims","page":"Insurance Claims","title":"Insurance Claims","text":"","category":"section"},{"location":"tutorials/iclaims.html","page":"Insurance Claims","title":"Insurance Claims","text":"This tutorial uses AutoGP to discover time series models of weekly insurance claims data.","category":"page"},{"location":"tutorials/iclaims.html","page":"Insurance Claims","title":"Insurance Claims","text":"import AutoGP\nimport CSV\nimport Dates\nimport DataFrames\n\nusing PyPlot: plt","category":"page"},{"location":"tutorials/iclaims.html","page":"Insurance Claims","title":"Insurance Claims","text":"We first load the iclaims.csv dataset from disk.  Since the data is positive we apply a log transform and perform all modeling in this transformed space.","category":"page"},{"location":"tutorials/iclaims.html","page":"Insurance Claims","title":"Insurance Claims","text":"data = CSV.File(\"assets/iclaims.csv\"; header=[:ds, :y], types=Dict(:ds=>Dates.Date, :y=>Float64));\ndf = DataFrames.DataFrame(data)\ndf[:,\"y\"] = log.(df[:,\"y\"])\nshow(df)","category":"page"},{"location":"tutorials/iclaims.html","page":"Insurance Claims","title":"Insurance Claims","text":"\u001b[1m     \u001b[0m│ ds          y\n─────┼─────────────────────\n   1 │ 2010-01-03  13.3866\n   2 │ 2010-01-10  13.6242\n   3 │ 2010-01-17  13.3987\n   4 │ 2010-01-24  13.1375\n   5 │ 2010-01-31  13.1968\n   6 │ 2010-02-07  13.147\n   7 │ 2010-02-14  13.0859\n   8 │ 2010-02-21  13.035\n   9 │ 2010-02-28  13.0704\n  10 │ 2010-03-07  13.0448\n  11 │ 2010-03-14  12.9924\n  12 │ 2010-03-21  12.9314\n  ⋮  │     ⋮          ⋮\n 433 │ 2018-04-15  12.3287\n 434 │ 2018-04-22  12.2068\n 435 │ 2018-04-29  12.1359\n 436 │ 2018-05-06  12.1562\n 437 │ 2018-05-13  12.1819\n 438 │ 2018-05-20  12.2407\n 439 │ 2018-05-27  12.2202\n 440 │ 2018-06-03  12.1628\n 441 │ 2018-06-10  12.289\n 442 │ 2018-06-17  12.2357\n 443 │ 2018-06-24  12.3139\n\u001b[36m           420 rows omitted\u001b[0m","category":"page"},{"location":"tutorials/iclaims.html","page":"Insurance Claims","title":"Insurance Claims","text":"Let's hold out the final 100 weeks of observations to serve as test data.","category":"page"},{"location":"tutorials/iclaims.html","page":"Insurance Claims","title":"Insurance Claims","text":"n_test = 100\nn_train = DataFrames.nrow(df) - n_test\ndf_train = df[1:end-n_test, :]\ndf_test = df[end-n_test+1:end, :]\n\nfig, ax = plt.subplots()\nax.scatter(df_train.ds, df_train.y, marker=\".\", color=\"k\", label=\"Observed Data\")\nax.scatter(df_test.ds, df_test.y, marker=\".\", color=\"r\", label=\"Test Data\")\nax.legend()\n\nfig.set_size_inches((20, 10))","category":"page"},{"location":"tutorials/iclaims.html","page":"Insurance Claims","title":"Insurance Claims","text":"(Image: png)","category":"page"},{"location":"tutorials/iclaims.html","page":"Insurance Claims","title":"Insurance Claims","text":"The next step is to initialize an AutoGP.GPModel instance and fit the model using sequential Monte Carlo structure learning.","category":"page"},{"location":"tutorials/iclaims.html","page":"Insurance Claims","title":"Insurance Claims","text":"model = AutoGP.GPModel(df_train.ds, df_train.y; n_particles=8);","category":"page"},{"location":"tutorials/iclaims.html","page":"Insurance Claims","title":"Insurance Claims","text":"AutoGP.seed!(10)\nschedule = AutoGP.Schedule.linear_schedule(n_train, .20)\nAutoGP.fit_smc!(model; schedule=schedule, n_mcmc=50, n_hmc=10, shuffle=true, adaptive_resampling=false, verbose=true);","category":"page"},{"location":"tutorials/iclaims.html","page":"Insurance Claims","title":"Insurance Claims","text":"Running SMC round 69/343\nParticle Weights: [3.38e-32, 4.11e-20, 7.26e-29, 1.02e-44, 4.74e-57, 9.10e-04, 9.99e-01, 2.71e-17]\nParticle ESS: 0.1252276603207894\nresampled true\naccepted MCMC[4/50] HMC[40/40]\naccepted MCMC[6/50] HMC[48/50]\naccepted MCMC[8/50] HMC[62/64]\naccepted MCMC[7/50] HMC[61/63]\naccepted MCMC[7/50] HMC[68/69]\naccepted MCMC[9/50] HMC[73/77]\naccepted MCMC[12/50] HMC[92/97]\naccepted MCMC[14/50] HMC[114/118]\nRunning SMC round 138/343\nParticle Weights: [1.10e-01, 1.39e-01, 1.98e-01, 1.71e-01, 1.66e-01, 1.78e-01, 1.80e-02, 2.08e-02]\nParticle ESS: 0.7836676673226702\nresampled true\naccepted MCMC[2/50] HMC[3/5]\naccepted MCMC[3/50] HMC[15/17]\naccepted MCMC[13/50] HMC[0/13]\naccepted MCMC[15/50] HMC[0/15]\naccepted MCMC[8/50] HMC[17/24]\naccepted MCMC[8/50] HMC[12/20]\naccepted MCMC[12/50] HMC[20/32]\naccepted MCMC[16/50] HMC[62/73]\nRunning SMC round 207/343\nParticle Weights: [1.21e-18, 1.72e-18, 1.00e+00, 3.00e-20, 1.48e-12, 2.59e-17, 8.86e-18, 1.49e-17]\nParticle ESS: 0.12500000000036948\nresampled true\naccepted MCMC[8/50] HMC[0/8]\naccepted MCMC[10/50] HMC[0/10]\naccepted MCMC[10/50] HMC[0/10]\naccepted MCMC[12/50] HMC[0/12]\naccepted MCMC[11/50] HMC[0/11]\naccepted MCMC[12/50] HMC[0/12]\naccepted MCMC[13/50] HMC[0/13]\naccepted MCMC[16/50] HMC[2/18]\nRunning SMC round 276/343\nParticle Weights: [1.52e-01, 1.63e-04, 1.51e-01, 4.79e-01, 1.04e-01, 9.47e-02, 1.76e-02, 1.36e-03]\nParticle ESS: 0.42322820857724425\nresampled true\naccepted MCMC[7/50] HMC[0/7]\naccepted MCMC[11/50] HMC[0/11]\naccepted MCMC[11/50] HMC[1/12]\naccepted MCMC[12/50] HMC[0/12]\naccepted MCMC[12/50] HMC[0/12]\naccepted MCMC[13/50] HMC[1/14]\naccepted MCMC[19/50] HMC[0/19]\naccepted MCMC[20/50] HMC[0/20]\nRunning SMC round 343/343\nParticle Weights: [4.25e-03, 3.87e-04, 5.39e-03, 5.37e-03, 2.16e-04, 5.35e-01, 4.40e-01, 9.31e-03]\nParticle ESS: 0.2603461961652077\naccepted MCMC[10/50] HMC[0/10]\naccepted MCMC[10/50] HMC[0/10]\naccepted MCMC[12/50] HMC[0/12]\naccepted MCMC[14/50] HMC[0/14]\naccepted MCMC[13/50] HMC[1/14]\naccepted MCMC[16/50] HMC[0/16]\naccepted MCMC[14/50] HMC[1/15]\naccepted MCMC[17/50] HMC[0/17]","category":"page"},{"location":"tutorials/iclaims.html","page":"Insurance Claims","title":"Insurance Claims","text":"Plotting the forecasts from each particle reflects the structural uncertainty.  7/8 particles have inferred a periodic component (AutoGP.GP.Periodic) with additive linear trend AutoGP.GP.Linear. 1/8 of the particles has inferred a sum of a periodic kernel and gamma exponential (AutoGP.GP.GammaExponential) kernel, which is stationary but not \"smooth\" (formally, not mean-square differentiable).","category":"page"},{"location":"tutorials/iclaims.html","page":"Insurance Claims","title":"Insurance Claims","text":"# Generate in-sample and future predictions.\nds_future = range(start=df_test.ds[end]+Dates.Week(1), step=Dates.Week(1), length=54*10)\nds_query = vcat(df_train.ds, df_test.ds, ds_future)\nforecasts = AutoGP.predict(model, ds_query; quantiles=[0.025, 0.975]);\nweights = AutoGP.particle_weights(model)\n\n# Plot the data.\nfig, ax = plt.subplots()\nax.scatter(df_train.ds, df_train.y, marker=\".\", color=\"k\", label=\"Observed Data\")\nax.scatter(df_test.ds, df_test.y, marker=\".\", color=\"r\", label=\"Test Data\")\n\n# Plot the forecasts from each particle.\nfor i=1:AutoGP.num_particles(model)\n    subdf = forecasts[forecasts.particle.==i,:]\n    ax.plot(subdf[!,\"ds\"], subdf[!,\"y_mean\"], color=\"k\", linewidth=.1)\n    ax.fill_between(\n        subdf.ds, subdf[!,\"y_0.025\"], subdf[!,\"y_0.975\"];\n        color=\"tab:blue\", alpha=0.025)\nend\n\n# Plot the grand mean.\nmvn = AutoGP.predict_mvn(model, ds_query)\nax.plot(ds_query, AutoGP.Distributions.mean(mvn), color=\"k\");\n\nfig.set_size_inches((20, 10))","category":"page"},{"location":"tutorials/iclaims.html","page":"Insurance Claims","title":"Insurance Claims","text":"(Image: png)","category":"page"},{"location":"tutorials/iclaims.html","page":"Insurance Claims","title":"Insurance Claims","text":"for (w, k) in zip(AutoGP.particle_weights(model), AutoGP.covariance_kernels(model))\n    println(\"Particle weight $(w)\")\n    display(k)\nend","category":"page"},{"location":"tutorials/iclaims.html","page":"Insurance Claims","title":"Insurance Claims","text":"Particle weight 0.004250523793201452\n\n\n\n＋\n├── LIN(0.43; 0.06, 0.40)\n└── PER(0.27, 0.15; 0.12)\n\n\n\nParticle weight 0.00038679558572478394\n\n\n\n＋\n├── LIN(0.21; 0.48, 0.28)\n└── PER(0.31, 0.15; 0.14)\n\n\n\nParticle weight 0.0053919770385277765\n\n\n\n＋\n├── PER(0.27, 0.15; 0.12)\n└── LIN(0.10; 0.11, 1.58)\n\n\n\nParticle weight 0.005373167226484039\n\n\n\n＋\n├── LIN(0.55; 0.04, 0.06)\n└── PER(0.27, 0.15; 0.12)\n\n\n\nParticle weight 0.00021615764118894197\n\n\n\n＋\n├── LIN(0.45; 0.05, 0.18)\n└── PER(0.27, 0.15; 0.12)\n\n\n\nParticle weight 0.5354267607554092\n\n\n\n＋\n├── PER(0.27, 0.15; 0.12)\n└── LIN(0.22; 0.19, 0.18)\n\n\n\nParticle weight 0.43964210411447696\n\n\n\n＋\n├── LIN(0.09; 0.12, 0.11)\n└── PER(0.23, 0.15; 0.11)\n\n\n\nParticle weight 0.00931251384499995\n\n\n\n＋\n├── GE(1.72, 1.66; 0.13)\n└── PER(0.29, 0.15; 0.12)","category":"page"},{"location":"tutorials/iclaims.html","page":"Insurance Claims","title":"Insurance Claims","text":"We can also query the overall quantiles of the predictive distribution over new data by using AutoGP.predict_quantile.","category":"page"},{"location":"tutorials/iclaims.html","page":"Insurance Claims","title":"Insurance Claims","text":"# Obtain overall quantiles.\nquantiles_lo, = AutoGP.predict_quantile.(Ref(model), Ref(ds_query), .025, tol=1e-6)\nquantiles_md, = AutoGP.predict_quantile.(Ref(model), Ref(ds_query), .50, tol=1e-6)\nquantiles_hi, = AutoGP.predict_quantile.(Ref(model), Ref(ds_query), .975, tol=1e-6)\n\n# Plot the combined predictions.\nfig, ax = plt.subplots()\nax.scatter(df_train.ds, df_train.y, marker=\".\", color=\"k\", label=\"Observed Data\")\nax.scatter(df_test.ds, df_test.y, marker=\".\", color=\"r\", label=\"Test Data\")\nax.plot(ds_query, quantiles_md, color=\"k\", linewidth=1)\nax.fill_between(ds_query, quantiles_lo, quantiles_hi, color=\"tab:blue\", alpha=.5)\nfig.set_size_inches((20, 10))","category":"page"},{"location":"tutorials/iclaims.html","page":"Insurance Claims","title":"Insurance Claims","text":"(Image: png)","category":"page"},{"location":"tutorials/iclaims.html","page":"Insurance Claims","title":"Insurance Claims","text":"note: Note\nMean forecasts, quantile forecasts, and probability densities values obtained via AutoGP.predict and AutoGP.predict_proba are all in the transformed (log space).  Only quantile forecasts can be transformed back to direct space via exp.  Converting mean forecasts and probability densities can be performed by using the Distributions.MvLogNormal constructor, as demonstrated below.","category":"page"},{"location":"tutorials/iclaims.html","page":"Insurance Claims","title":"Insurance Claims","text":"import Distributions\nlog_mvn_components = [Distributions.MvLogNormal(d) for d in Distributions.components(mvn)]\nlog_mvn_weights = Distributions.probs(mvn)\nlog_mvn = Distributions.MixtureModel(log_mvn_components, log_mvn_weights);","category":"page"},{"location":"tutorials/iclaims.html","page":"Insurance Claims","title":"Insurance Claims","text":"fig, ax = plt.subplots()\nax.plot(ds_query, Distributions.mean(log_mvn), color=\"tab:blue\", label=\"Correct Mean Forecasts in Direct Space\")\nax.plot(ds_query, exp.(Distributions.mean(mvn)), color=\"k\", label=\"Incorrect Mean Forecasts in Direct Space\")\nax.axvline(df_test.ds[end], color=\"k\", linestyle=\"--\")\nax.legend()","category":"page"},{"location":"tutorials/iclaims.html","page":"Insurance Claims","title":"Insurance Claims","text":"(Image: png)","category":"page"},{"location":"tutorials/iclaims.html","page":"Insurance Claims","title":"Insurance Claims","text":"PyObject <matplotlib.legend.Legend object at 0x7f5b068d02e0>","category":"page"},{"location":"tutorials/iclaims.html","page":"Insurance Claims","title":"Insurance Claims","text":"The difference between the blue and black curves is too small to observe on the scale above; let us plot the bias that arises from doing a naive transformation of the predictive mean.","category":"page"},{"location":"tutorials/iclaims.html","page":"Insurance Claims","title":"Insurance Claims","text":"fig, ax = plt.subplots()\nax.plot(ds_query, Distributions.mean(log_mvn) - exp.(Distributions.mean(mvn)));","category":"page"},{"location":"tutorials/iclaims.html","page":"Insurance Claims","title":"Insurance Claims","text":"(Image: png)","category":"page"},{"location":"tutorials/overview.html#Overview","page":"Overview","title":"Overview","text":"","category":"section"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"This tutorial demonstrates the basic capabilities of the AutoGP package.","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"import AutoGP","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"import CSV\nimport Dates\nimport DataFrames\n\nusing PyPlot: plt","category":"page"},{"location":"tutorials/overview.html#Loading-Data","page":"Overview","title":"Loading Data","text":"","category":"section"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"The first step is to load a dataset from disk. The tsdl.161.csv file, obtained from the Time Series Data Library, has two columns:","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"ds indicates time stamps.\ny indicates measured time series values.","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"In the call to CSV.File, we explicitly set the type of the ds column to Dates.Date, permitted types for time indexes are types T <: Real and T < :Dates.TimeType, see AutoGP.IndexType.","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"data = CSV.File(\"assets/tsdl.161.csv\"; header=[:ds, :y], types=Dict(:ds=>Dates.Date, :y=>Float64));\ndf = DataFrames.DataFrame(data)\nshow(df)","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"\u001b[1m     \u001b[0m│ ds          y\n─────┼───────────────────\n   1 │ 1949-01-01  112.0\n   2 │ 1949-02-01  118.0\n   3 │ 1949-03-01  132.0\n   4 │ 1949-04-01  129.0\n   5 │ 1949-05-01  121.0\n   6 │ 1949-06-01  135.0\n   7 │ 1949-07-01  148.0\n   8 │ 1949-08-01  148.0\n   9 │ 1949-09-01  136.0\n  10 │ 1949-10-01  119.0\n  11 │ 1949-11-01  104.0\n  12 │ 1949-12-01  118.0\n  ⋮  │     ⋮         ⋮\n 134 │ 1960-02-01  391.0\n 135 │ 1960-03-01  419.0\n 136 │ 1960-04-01  461.0\n 137 │ 1960-05-01  472.0\n 138 │ 1960-06-01  535.0\n 139 │ 1960-07-01  622.0\n 140 │ 1960-08-01  606.0\n 141 │ 1960-09-01  508.0\n 142 │ 1960-10-01  461.0\n 143 │ 1960-11-01  390.0\n 144 │ 1960-12-01  432.0\n\u001b[36m         121 rows omitted\u001b[0m","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"We next split the data into a training set and test set.","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"n_test = 18\nn_train = DataFrames.nrow(df) - n_test\ndf_train = df[1:end-n_test, :]\ndf_test = df[end-n_test+1:end, :]\n\nfig, ax = plt.subplots(figsize=(10,5))\nax.scatter(df_train.ds, df_train.y, marker=\".\", color=\"k\", label=\"Observed Data\")\nax.scatter(df_test.ds, df_test.y, marker=\".\", color=\"r\", label=\"Test Data\")\nax.legend();","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"(Image: png)","category":"page"},{"location":"tutorials/overview.html#Creating-an-AutoGP-Model","page":"Overview","title":"Creating an AutoGP Model","text":"","category":"section"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"Julia natively supports multiprocessing, which greatly improves performance for embarrassingly parallel computations in AutoGP. The number of threads available to Julia can be set using the JULIA_NUM_THREADS=[nthreads] environment variable or invoking julia -t [nthreads] from the command line.","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"Threads.nthreads()","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"8","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"We next initialize a AutoGP.GPModel, which will enable us to automatically discover an ensemble of Gaussian process covariance kernels for modeling the time series data. Initially, the model structures and parameters are sampled from the prior.  The n_particles argument is optional and specifices the number of particles for sequential Monte Carlo inference.","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"model = AutoGP.GPModel(df_train.ds, df_train.y; n_particles=6);","category":"page"},{"location":"tutorials/overview.html#Generating-Prior-Forecasts","page":"Overview","title":"Generating Prior Forecasts","text":"","category":"section"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"Calling AutoGP.covariance_kernels returns the ensemble of covariance kernel structures and parameters, whose weights are given by AutoGP.particle_weights. These model structures have not yet been fitted to the data, so we are essentially importance sampling the posterior over structures and parameters given data by using the prior distribution as the proposal.","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"weights = AutoGP.particle_weights(model)\nkernels = AutoGP.covariance_kernels(model)\nfor (i, (k, w)) in enumerate(zip(kernels, weights))\n    println(\"Model $(i), Weight $(w)\")\n    Base.display(k)\nend","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"Model 1, Weight 0.26142074394894477\n\n\n\nCP(0.19055667126219877, 0.001)\n├── LIN(1.26; 3.19, 0.06)\n└── PER(0.11, 0.52; 0.13)\n\n\n\nModel 2, Weight 4.0257929388061755e-27\n\n\n\nLIN(0.09; 0.52, 0.20)\n\n\n\nModel 3, Weight 1.7975106803349286e-9\n\n\n\nPER(1.37, 0.30; 1.04)\n\n\n\nModel 4, Weight 1.1645819463329054e-21\n\n\n\nGE(0.42, 1.00; 0.04)\n\n\n\nModel 5, Weight 7.24814391228416e-39\n\n\n\n×\n├── GE(0.09, 0.54; 1.82)\n└── PER(0.18, 0.14; 0.42)\n\n\n\nModel 6, Weight 0.7385792542535452\n\n\n\nGE(0.13, 0.29; 0.11)","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"Forecasts are obtained using AutoGP.predict, which takes in a model, a list of time points ds (which we specify to be the observed time points, the test time points, and 36 months of future time points). We also specify a list of quantiles for obtaining prediction intervals. The return value is a DataFrames.DataFrame object that show the particle id, particle weight, and predictions from each of the particles in model.","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"ds_future = range(start=df.ds[end]+Dates.Month(1), step=Dates.Month(1), length=36)\nds_query = vcat(df_train.ds, df_test.ds, ds_future)\nforecasts = AutoGP.predict(model, ds_query; quantiles=[0.025, 0.975])\nshow(forecasts)","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"\u001b[1m      \u001b[0m│ ds          particle  weight    y_0.025   y_0.975  y_mean\n──────┼────────────────────────────────────────────────────────────\n    1 │ 1949-01-01         1  0.261421  -83.3461  347.688  132.171\n    2 │ 1949-02-01         1  0.261421  -83.1647  347.645  132.24\n    3 │ 1949-03-01         1  0.261421  -83.0097  347.615  132.303\n    4 │ 1949-04-01         1  0.261421  -82.8478  347.592  132.372\n    5 │ 1949-05-01         1  0.261421  -82.7009  347.579  132.439\n    6 │ 1949-06-01         1  0.261421  -82.5592  347.576  132.508\n    7 │ 1949-07-01         1  0.261421  -82.4319  347.583  132.575\n    8 │ 1949-08-01         1  0.261421  -82.3105  347.6    132.645\n    9 │ 1949-09-01         1  0.261421  -82.1993  347.627  132.714\n   10 │ 1949-10-01         1  0.261421  -82.1016  347.664  132.781\n   11 │ 1949-11-01         1  0.261421  -82.0108  347.711  132.85\n   12 │ 1949-12-01         1  0.261421  -81.9328  347.767  132.917\n  ⋮   │     ⋮          ⋮         ⋮         ⋮         ⋮        ⋮\n 1070 │ 1963-02-01         6  0.738579  -71.2452  656.635  292.695\n 1071 │ 1963-03-01         6  0.738579  -71.8672  656.291  292.212\n 1072 │ 1963-04-01         6  0.738579  -72.5386  655.92   291.691\n 1073 │ 1963-05-01         6  0.738579  -73.1719  655.57   291.199\n 1074 │ 1963-06-01         6  0.738579  -73.8099  655.217  290.703\n 1075 │ 1963-07-01         6  0.738579  -74.4122  654.883  290.235\n 1076 │ 1963-08-01         6  0.738579  -75.0194  654.546  289.763\n 1077 │ 1963-09-01         6  0.738579  -75.6119  654.218  289.303\n 1078 │ 1963-10-01         6  0.738579  -76.1718  653.907  288.868\n 1079 │ 1963-11-01         6  0.738579  -76.7368  653.594  288.428\n 1080 │ 1963-12-01         6  0.738579  -77.2711  653.297  288.013\n\u001b[36m                                                  1057 rows omitted\u001b[0m","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"Let us visualize the forecasts before model fitting. The model clearly underfits the data.","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"fig, ax = plt.subplots(figsize=(10,5))\n\nax.scatter(df_train.ds, df_train.y, marker=\".\", color=\"k\", label=\"Observed Data\")\nax.scatter(df_test.ds, df_test.y, marker=\".\", color=\"r\", label=\"Test Data\")\n\nfor i=1:AutoGP.num_particles(model)\n    subdf = forecasts[forecasts.particle.==i,:]\n    ax.plot(subdf[!,\"ds\"], subdf[!,\"y_mean\"], color=\"k\", linewidth=.5)\n    ax.fill_between(\n        subdf.ds, subdf[!,\"y_0.025\"], subdf[!,\"y_0.975\"];\n        color=\"tab:blue\", alpha=0.05)\nend","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"(Image: png)","category":"page"},{"location":"tutorials/overview.html#Model-Fitting-via-SMC","page":"Overview","title":"Model Fitting via SMC","text":"","category":"section"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"The next step is to fit the model to the observed data. There are three fitting algorithms available.  We will use AutoGP.fit_smc! which leverages sequential Monte Carlo structure learning to infer the covariance kernel structures and parameters.","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"The annealing schedule below adds roughly 10% of the observed data at each step, with 100 MCMC rejuvenation steps over the structure and 10 Hamiltonian Monte Carlo steps for the parameters. Using verbose=true will print some statistics about the acceptance rates of difference MCMC and HMC moves that are performed within the SMC learning algorithm.","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"AutoGP.seed!(6)\nAutoGP.fit_smc!(model; schedule=AutoGP.Schedule.linear_schedule(n_train, .10), n_mcmc=75, n_hmc=10, verbose=true);","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"Running SMC round 13/126\nweights [2.14e-01, 5.85e-02, 2.05e-01, 5.23e-01, 1.09e-04, 1.51e-04]\nresampled true\naccepted MCMC[12/75] HMC[71/78]\naccepted MCMC[9/75] HMC[64/69]\naccepted MCMC[19/75] HMC[122/132]\naccepted MCMC[22/75] HMC[121/136]\naccepted MCMC[22/75] HMC[89/107]\naccepted MCMC[27/75] HMC[191/202]\nRunning SMC round 26/126\nweights [8.08e-03, 4.24e-01, 1.10e-03, 1.92e-02, 3.97e-01, 1.50e-01]\nresampled true\naccepted MCMC[7/75] HMC[55/58]\naccepted MCMC[6/75] HMC[37/40]\naccepted MCMC[6/75] HMC[43/47]\naccepted MCMC[12/75] HMC[52/60]\naccepted MCMC[18/75] HMC[84/97]\naccepted MCMC[21/75] HMC[112/125]\nRunning SMC round 39/126\nweights [1.99e-02, 1.74e-01, 3.01e-02, 4.08e-02, 1.95e-02, 7.16e-01]\nresampled true\naccepted MCMC[0/75] HMC[0/0]\naccepted MCMC[1/75] HMC[6/7]\naccepted MCMC[5/75] HMC[43/44]\naccepted MCMC[13/75] HMC[50/61]\naccepted MCMC[10/75] HMC[64/71]\naccepted MCMC[17/75] HMC[95/105]\nRunning SMC round 52/126\nweights [1.66e-03, 8.13e-02, 8.69e-02, 1.65e-01, 4.54e-02, 6.20e-01]\nresampled true\naccepted MCMC[3/75] HMC[24/25]\naccepted MCMC[4/75] HMC[24/27]\naccepted MCMC[11/75] HMC[25/36]\naccepted MCMC[17/75] HMC[111/122]\naccepted MCMC[23/75] HMC[62/84]\naccepted MCMC[24/75] HMC[92/114]\nRunning SMC round 65/126\nweights [3.87e-02, 5.07e-01, 7.59e-03, 1.50e-01, 5.29e-02, 2.44e-01]\nresampled true\naccepted MCMC[0/75] HMC[0/0]\naccepted MCMC[18/75] HMC[7/25]\naccepted MCMC[20/75] HMC[11/31]\naccepted MCMC[28/75] HMC[27/55]\naccepted MCMC[26/75] HMC[25/51]\naccepted MCMC[27/75] HMC[35/62]\nRunning SMC round 78/126\nweights [5.42e-06, 2.46e-04, 1.94e-04, 3.56e-04, 9.42e-01, 5.75e-02]\nresampled true\naccepted MCMC[14/75] HMC[0/14]\naccepted MCMC[14/75] HMC[0/14]\naccepted MCMC[17/75] HMC[0/17]\naccepted MCMC[15/75] HMC[2/17]\naccepted MCMC[26/75] HMC[2/28]\naccepted MCMC[18/75] HMC[0/18]\nRunning SMC round 91/126\nweights [5.03e-02, 1.90e-04, 3.17e-01, 9.72e-02, 1.74e-01, 3.61e-01]\nresampled false\naccepted MCMC[16/75] HMC[0/16]\naccepted MCMC[15/75] HMC[0/15]\naccepted MCMC[12/75] HMC[0/12]\naccepted MCMC[16/75] HMC[0/16]\naccepted MCMC[18/75] HMC[0/18]\naccepted MCMC[28/75] HMC[1/29]\nRunning SMC round 104/126\nweights [1.14e-02, 6.30e-08, 7.42e-02, 6.76e-01, 1.48e-01, 9.09e-02]\nresampled true\naccepted MCMC[15/75] HMC[0/15]\naccepted MCMC[14/75] HMC[0/14]\naccepted MCMC[12/75] HMC[0/12]\naccepted MCMC[16/75] HMC[0/16]\naccepted MCMC[15/75] HMC[1/16]\naccepted MCMC[30/75] HMC[0/30]\nRunning SMC round 117/126\nweights [3.84e-01, 4.30e-02, 2.19e-01, 1.02e-01, 1.59e-01, 9.31e-02]\nresampled false\naccepted MCMC[9/75] HMC[0/9]\naccepted MCMC[15/75] HMC[0/15]\naccepted MCMC[20/75] HMC[1/21]\naccepted MCMC[15/75] HMC[0/15]\naccepted MCMC[17/75] HMC[0/17]\naccepted MCMC[20/75] HMC[0/20]\nRunning SMC round 126/126\nweights [5.19e-01, 3.88e-02, 5.85e-02, 1.22e-01, 9.97e-02, 1.62e-01]\naccepted MCMC[10/75] HMC[0/10]\naccepted MCMC[15/75] HMC[0/15]\naccepted MCMC[13/75] HMC[1/14]\naccepted MCMC[20/75] HMC[0/20]\naccepted MCMC[17/75] HMC[0/17]\naccepted MCMC[24/75] HMC[0/24]","category":"page"},{"location":"tutorials/overview.html#Generating-Posterior-Forecasts","page":"Overview","title":"Generating Posterior Forecasts","text":"","category":"section"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"Having the fit data, we can now inspect the ensemble of posterior structures, parameters, and predictions.","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"ds_future = range(start=df_test.ds[end]+Dates.Month(1), step=Dates.Month(1), length=36)\nds_query = vcat(df_train.ds, df_test.ds, ds_future)\nforecasts = AutoGP.predict(model, ds_query; quantiles=[0.025, 0.975]);\nshow(forecasts)","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"\u001b[1m      \u001b[0m│ ds          particle  weight    y_0.025   y_0.975   y_mean\n──────┼─────────────────────────────────────────────────────────────\n    1 │ 1949-01-01         1  0.518786   95.7778   124.096  109.937\n    2 │ 1949-02-01         1  0.518786  105.533    132.895  119.214\n    3 │ 1949-03-01         1  0.518786  118.461    145.764  132.113\n    4 │ 1949-04-01         1  0.518786  112.317    139.654  125.985\n    5 │ 1949-05-01         1  0.518786  106.34     133.628  119.984\n    6 │ 1949-06-01         1  0.518786  118.932    146.186  132.559\n    7 │ 1949-07-01         1  0.518786  131.405    158.626  145.016\n    8 │ 1949-08-01         1  0.518786  132.641    159.89   146.266\n    9 │ 1949-09-01         1  0.518786  122.15     149.355  135.753\n   10 │ 1949-10-01         1  0.518786  104.155    131.338  117.746\n   11 │ 1949-11-01         1  0.518786   92.5327   119.705  106.119\n   12 │ 1949-12-01         1  0.518786  102.479    129.31   115.894\n  ⋮   │     ⋮          ⋮         ⋮         ⋮         ⋮         ⋮\n 1070 │ 1963-02-01         6  0.162471  264.78     700.582  482.681\n 1071 │ 1963-03-01         6  0.162471  334.241    779.225  556.733\n 1072 │ 1963-04-01         6  0.162471  318.448    773.472  545.96\n 1073 │ 1963-05-01         6  0.162471  345.825    809.683  577.754\n 1074 │ 1963-06-01         6  0.162471  449.011    921.948  685.479\n 1075 │ 1963-07-01         6  0.162471  556.444   1049.52   802.984\n 1076 │ 1963-08-01         6  0.162471  576.012   1085.92   830.965\n 1077 │ 1963-09-01         6  0.162471  408.099    932.331  670.215\n 1078 │ 1963-10-01         6  0.162471  299.235    837.312  568.274\n 1079 │ 1963-11-01         6  0.162471  216.275    768.47   492.373\n 1080 │ 1963-12-01         6  0.162471  240.204    805.597  522.9\n\u001b[36m                                                   1057 rows omitted\u001b[0m","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"The plot below reflects posterior uncertainty as to whether the linear componenet will persist or the data will revert to the mean.","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"fig, ax = plt.subplots(figsize=(10,5))\nax.scatter(df_train.ds, df_train.y, marker=\".\", color=\"k\", label=\"Observed Data\")\nax.scatter(df_test.ds, df_test.y, marker=\".\", color=\"r\", label=\"Test Data\")\n\nfor i=1:AutoGP.num_particles(model)\n    subdf = forecasts[forecasts.particle.==i,:]\n    ax.plot(subdf[!,\"ds\"], subdf[!,\"y_mean\"], color=\"k\", linewidth=.5)\n    ax.fill_between(\n        subdf.ds, subdf[!,\"y_0.025\"], subdf[!,\"y_0.975\"];\n        color=\"tab:blue\", alpha=0.05)\nend","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"(Image: png)","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"We can also inspect the discovered kernel structures and their weights.","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"weights = AutoGP.particle_weights(model)\nkernels = AutoGP.covariance_kernels(model)\nfor (i, (k, w)) in enumerate(zip(kernels, weights))\n    println(\"Model $(i), Weight $(w)\")\n    display(k)\nend","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"Model 1, Weight 0.5187856913664618\n\n\n\n＋\n├── ×\n│   ├── LIN(0.58; 0.56, 0.19)\n│   └── ＋\n│       ├── LIN(0.19; 0.92, 0.79)\n│       └── ×\n│           ├── LIN(0.05; 0.02, 0.38)\n│           └── ＋\n│               ├── PER(0.78, 0.10; 0.20)\n│               └── GE(0.72, 1.43; 0.11)\n└── ×\n    ├── LIN(0.16; 0.55, 0.51)\n    └── ×\n        ├── LIN(0.06; 0.16, 0.08)\n        └── LIN(0.22; 0.07, 0.46)\n\n\n\nModel 2, Weight 0.0388018393363426\n\n\n\n×\n├── LIN(0.38; 0.08, 0.92)\n└── ＋\n    ├── ×\n    │   ├── LIN(0.58; 0.13, 0.07)\n    │   └── GE(0.56, 1.51; 0.69)\n    └── ×\n        ├── LIN(0.05; 0.10, 0.70)\n        └── PER(0.48, 0.10; 0.17)\n\n\n\nModel 3, Weight 0.05854237186951741\n\n\n\n×\n├── LIN(0.06; 0.12, 0.55)\n└── ＋\n    ├── LIN(0.06; 0.83, 0.20)\n    └── ×\n        ├── LIN(0.05; 0.03, 0.37)\n        └── ＋\n            ├── PER(0.35, 0.10; 0.13)\n            └── GE(0.58, 1.50; 0.20)\n\n\n\nModel 4, Weight 0.12171996350432317\n\n\n\n×\n├── LIN(0.07; 0.12, 0.50)\n└── ＋\n    ├── ×\n    │   ├── LIN(0.07; 0.08, 0.19)\n    │   └── GE(0.56, 1.51; 0.69)\n    └── ×\n        ├── LIN(0.30; 0.13, 0.27)\n        └── PER(0.48, 0.10; 0.17)\n\n\n\nModel 5, Weight 0.09967934632668538\n\n\n\n×\n├── LIN(0.33; 0.08, 0.84)\n└── ＋\n    ├── ×\n    │   ├── LIN(0.40; 0.13, 0.07)\n    │   └── GE(0.69, 1.28; 0.33)\n    └── ×\n        ├── LIN(0.26; 0.10, 0.30)\n        └── ＋\n            ├── PER(0.54, 0.10; 0.14)\n            └── LIN(0.52; 0.59, 0.21)\n\n\n\nModel 6, Weight 0.16247078759667166\n\n\n\n×\n├── LIN(0.31; 0.12, 0.32)\n└── ＋\n    ├── ×\n    │   ├── LIN(0.22; 0.21, 0.23)\n    │   └── ＋\n    │       ├── ×\n    │       │   ├── LIN(0.30; 0.24, 0.92)\n    │       │   └── LIN(0.10; 1.53, 0.25)\n    │       └── ＋\n    │           ├── GE(0.23, 1.25; 0.06)\n    │           └── LIN(0.04; 0.34, 0.08)\n    └── ×\n        ├── LIN(0.38; 0.28, 0.27)\n        └── PER(0.48, 0.10; 0.17)","category":"page"},{"location":"tutorials/overview.html#Computing-Predictive-Probabilities","page":"Overview","title":"Computing Predictive Probabilities","text":"","category":"section"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"In addition to generating forecasts, the predictive probability of new data can be computed by using AutoGP.predict_proba. The table below shows that the particles in our collection are able to predict the future data with varying accuracy, illustrating the benefits of maintaining an ensemble of learned structures.","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"logps = AutoGP.predict_proba(model, df_test.ds, df_test.y);\nshow(logps)","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"\u001b[1m   \u001b[0m│ particle  weight     logp\n───┼───────────────────────────────\n 1 │        1  0.518786   -81.0226\n 2 │        2  0.0388018  -80.4687\n 3 │        3  0.0585424  -78.3537\n 4 │        4  0.12172    -79.2853\n 5 │        5  0.0996793  -79.5163\n 6 │        6  0.162471   -77.2665","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"It is also possible to directly access the underlying predictive distribution of new data at arbitrary time series values by using AutoGP.predict_mvn, which returns an instance of Distributions.MixtureModel. The Distributions.MvNormal object corresponding to each of the 7 particles in the mixture can be extracted using Distributions.components and the weights extracted using Distributions.probs.","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"Each MvNormal in the mixture has 18 dimensions corresponding to the length of df_test.ds.","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"mvn = AutoGP.predict_mvn(model, df_test.ds)","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"MixtureModel{Distributions.MvNormal}(K = 6)\ncomponents[1] (prior = 0.5188): FullNormal(\ndim: 18\nμ: [550.0466055631144, 543.5685832436789, 458.0672732626237, 387.83042059450264, 336.3274841090669, 378.84360283044555, 380.7405954069613, 370.5597600287218, 428.60605677444596, 417.8272939610252, 445.710229207111, 527.722968622132, 606.5232420193234, 592.7113473338969, 495.58143177869687, 415.9744653512393, 361.6811541092175, 410.86491959299735]\nΣ: [110.43133996791498 70.43337927366102 … 106.0193369583544 105.68013071952048; 70.43337927366102 157.2315611083291 … 175.7899821967875 176.06414206239037; … ; 106.0193369583544 175.7899821967875 … 1289.6029024068575 1266.7465844069018; 105.68013071952048 176.06414206239037 … 1266.7465844069018 1402.6718895178888]\n)\n\ncomponents[2] (prior = 0.0388): FullNormal(\ndim: 18\nμ: [550.0896881260433, 557.9183559988504, 451.7397701335289, 387.6936971300408, 330.34991142470057, 359.2956794851918, 378.39571104805384, 350.04141045569975, 413.64406521435524, 398.5566703647475, 422.68213778752005, 500.29404572255766, 582.8353873538858, 592.0046117753294, 465.11743221070196, 393.04897063058206, 329.267681609935, 358.99798544788507]\nΣ: [170.0085073382823 105.25012857653914 … 177.47794790860507 175.32855445033792; 105.25012857653914 238.13684073916644 … 293.4524054906246 290.979138749098; … ; 177.47794790860507 293.4524054906246 … 2416.364949433486 2381.6802393098687; 175.32855445033792 290.979138749098 … 2381.6802393098687 2639.691137455322]\n)\n\ncomponents[3] (prior = 0.0585): FullNormal(\ndim: 18\nμ: [544.0602736685337, 552.3149196264309, 443.2068512785211, 383.4411432273028, 333.1461848874234, 363.8639799436975, 380.29770568852405, 358.75477706207874, 423.7049992204387, 408.1716555603478, 445.55695338552516, 518.8219447058625, 591.4986944066081, 605.5457079908405, 470.62796484325577, 405.9325078335371, 358.7557037493446, 386.64043978162846]\nΣ: [169.219485798796 95.5607024083134 … 186.32839433051615 187.43906631934843; 95.5607024083134 226.17368143388276 … 296.7553075541498 297.9986486627047; … ; 186.32839433051615 296.7553075541498 … 2674.2672217311588 2576.455195366638; 187.43906631934843 297.9986486627047 … 2576.455195366638 2939.5711056844825]\n)\n\ncomponents[4] (prior = 0.1217): FullNormal(\ndim: 18\nμ: [543.6216230023799, 547.0922547835243, 446.04078249000014, 380.61770131619136, 324.29212851917504, 353.77691026184584, 371.1395407479827, 342.34364108088573, 404.0153377656334, 390.97432104186356, 413.05844950054205, 491.9302348550482, 568.3911582492668, 571.8976295358883, 452.94290011076225, 379.16919720465506, 317.0536108758739, 347.69198390460025]\nΣ: [190.70740795818799 154.7146370830134 … 264.57139974473904 263.6724500005631; 154.7146370830134 321.7637732335915 … 489.4185844326134 488.7990902096624; … ; 264.57139974473904 489.4185844326134 … 4640.723077886049 4724.747558194855; 263.6724500005631 488.7990902096624 … 4724.747558194855 5100.627801272796]\n)\n\ncomponents[5] (prior = 0.0997): FullNormal(\ndim: 18\nμ: [552.7093727527794, 563.9629154045222, 463.17633751362314, 397.0289509923992, 340.6980456518664, 369.3109007416625, 393.9776845927133, 364.4534810210352, 428.49669476215763, 419.62843208711365, 437.11650012898417, 518.4548245910088, 604.432782069657, 621.3790060721707, 502.57788998868597, 426.35148491965776, 363.78741844671123, 392.4909178791416]\nΣ: [163.93677598232696 101.84796443535812 … 150.19871047085653 148.021181862565; 101.84796443535812 230.96594095228826 … 248.28364894831142 246.73036788677837; … ; 150.19871047085653 248.28364894831142 … 1894.8528328890138 1837.9190390714505; 148.021181862565 246.73036788677837 … 1837.9190390714505 2060.0738352212666]\n)\n\ncomponents[6] (prior = 0.1625): FullNormal(\ndim: 18\nμ: [547.7766511058762, 554.0371551300294, 458.7231316164908, 395.94898562441665, 343.42395227426294, 375.81336956905994, 396.18401584832765, 371.80967668657166, 434.12203157338473, 424.18770474918983, 446.63575317375114, 526.406634220377, 606.4956206078521, 613.7056476449546, 502.8621955266556, 432.7728333122789, 375.92004227567435, 410.64183137650747]\nΣ: [155.68263652531186 105.08831951166466 … 153.16479636831943 151.89660431693395; 105.08831951166466 230.1409831228302 … 262.3355496966857 260.5635431735595; … ; 153.16479636831943 262.3355496966857 … 2040.7723016580135 2008.230949860343; 151.89660431693395 260.5635431735595 … 2008.230949860343 2220.8419553962394]\n)","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"Operations from the Distributions package can now be applied to the mvn object.","category":"page"},{"location":"tutorials/overview.html#Incorporating-New-Data","page":"Overview","title":"Incorporating New Data","text":"","category":"section"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"Online learning is supported by using AutoGP.add_data!, which lets us incorporate a new batch of observations.  Each particle's weight will be updated based on how well it predicts the new data (technically, the predictive probability it assigns to the new observations). Before adding new data, let us first look at the current particle weights.","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"AutoGP.particle_weights(model)","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"6-element Vector{Float64}:\n 0.5187856913664618\n 0.0388018393363426\n 0.05854237186951741\n 0.12171996350432317\n 0.09967934632668538\n 0.16247078759667166","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"Here are the forecasts and predictive probabilities of the test data under each particle in model.","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"using Printf: @sprintf","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"forecasts = AutoGP.predict(model, df_test.ds; quantiles=[0.025, 0.975])\n\nfig, axes = plt.subplots(ncols=6)\nfor i=1:AutoGP.num_particles(model)\n    axes[i].scatter(df_test.ds, df_test.y, marker=\".\", color=\"r\", label=\"Test Data\")\n    subdf = forecasts[forecasts.particle.==i,:]\n    axes[i].plot(subdf[!,\"ds\"], subdf[!,\"y_mean\"], color=\"k\", linewidth=.5)\n    axes[i].fill_between(\n        subdf.ds, subdf[!,\"y_0.025\"], subdf[!,\"y_0.975\"];\n        color=\"tab:blue\", alpha=0.1)\n    axes[i].set_title(\"$(i), logp $(@sprintf \"%1.2f\" logps[i,:logp])\")\n    axes[i].set_yticks([])\n    axes[i].set_ylim([0.8*minimum(df_test.y), 1.1*maximum(df_test.y)])\n    axes[i].tick_params(axis=\"x\", labelrotation=90)\nfig.set_size_inches(12, 5)\nfig.set_tight_layout(true)\nend","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"(Image: png)","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"Now let's incorporate the data and see what happens to the particle weights.","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"AutoGP.add_data!(model, df_test.ds, df_test.y)\nAutoGP.particle_weights(model)","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"6-element Vector{Float64}:\n 0.054480109584894444\n 0.007089917054484329\n 0.08867768509766472\n 0.07262818579191704\n 0.04720986321304457\n 0.7299142392579898","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"The particle weights have changed to reflect the fact that some particles are able to predict the new data better than others, which indicates they are able to better capture the underlying data generating process. The particles can be resampled using AutoGP.maybe_resample!, we will use an effective sample size of num_particles(model)/2 as the resampling criterion.","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"AutoGP.maybe_resample!(model, AutoGP.num_particles(model)/2)","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"true","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"Because the resampling critereon was met, the particles were resampled and now have equal weights.","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"AutoGP.particle_weights(model)","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"6-element Vector{Float64}:\n 0.16666666666666669\n 0.16666666666666669\n 0.16666666666666669\n 0.16666666666666669\n 0.16666666666666669\n 0.16666666666666669","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"The estimate of the marginal likelihood can be computed using AutoGP.log_marginal_likelihood_estimate.","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"AutoGP.log_marginal_likelihood_estimate(model)","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"214.57030788045827","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"Since we have added new data, we can update the particle structures and parameters by using AutoGP.mcmc_structure!. Note that this \"particle rejuvenation\" operation does not impact the weights. ","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"AutoGP.mcmc_structure!(model, 100, 10; verbose=true)","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"accepted MCMC[19/100] HMC[0/38]\naccepted MCMC[27/100] HMC[0/54]\naccepted MCMC[23/100] HMC[0/46]\naccepted MCMC[25/100] HMC[0/50]\naccepted MCMC[26/100] HMC[0/52]\naccepted MCMC[25/100] HMC[0/50]","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"Let's generate and plot the forecasts over the 36 month period again now that we have observed all the data. The prediction intervals are markedly narrower and it is more likely that the linear trend will persist rather than revert.","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"forecasts = AutoGP.predict(model, ds_query; quantiles=[0.025, 0.975]);\n\nfig, ax = plt.subplots(figsize=(10,5))\nax.scatter(df.ds, df.y, marker=\".\", color=\"k\", label=\"Observed Data\")\nfor i=1:AutoGP.num_particles(model)\n    subdf = forecasts[forecasts.particle.==i,:]\n    ax.plot(subdf[!,\"ds\"], subdf[!,\"y_mean\"], color=\"k\", linewidth=.5)\n    ax.fill_between(\n        subdf.ds, subdf[!,\"y_0.025\"], subdf[!,\"y_0.975\"];\n        color=\"tab:blue\", alpha=0.05)\nend","category":"page"},{"location":"tutorials/overview.html","page":"Overview","title":"Overview","text":"(Image: png)","category":"page"},{"location":"index.html#AutoGP.jl","page":"Home","title":"AutoGP.jl","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"A Julia package for learning the covariance structure of Gaussian process time series models.","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"(Image: png)","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"","category":"page"},{"location":"index.html#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"First, obtain Julia 1.8 or later, available here.","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"The AutoGP package can be installed with the Julia package manager. From the Julia REPL, type ] to enter the Pkg REPL mode and then run:","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"pkg> add AutoGP","category":"page"},{"location":"index.html#Tutorials","page":"Home","title":"Tutorials","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"Please refer to the Tutorials page.","category":"page"},{"location":"index.html#Citation","page":"Home","title":"Citation","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"@inproceedings{saad2023icml,\ntitle        = {Sequential {Monte} {Carlo} Learning for Time Series Structure Discovery},\nauthor       = {Saad, Feras A. and Patton, Brian J. and Hoffmann, Matthew D. and Saurous, Rif A. and Mansinghka, V. K.},\nbooktitle    = {Proceedings of the 40th International Conference on Machine Learning},\nseries       = {Proceedings of Machine Learning Research},\nvolume       = {202},\npages        = {29473--29489},\nyear         = {2023},\npublisher    = {PMLR},\n}","category":"page"},{"location":"index.html#License","page":"Home","title":"License","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"AutoGP.jl is licensed under the Apache License, Version 2.0; refer to LICENSE.","category":"page"},{"location":"utils.html#Utilities","page":"Utilities","title":"Utilities","text":"","category":"section"},{"location":"utils.html#AutoGP.Callbacks","page":"Utilities","title":"AutoGP.Callbacks","text":"","category":"section"},{"location":"utils.html","page":"Utilities","title":"Utilities","text":"Modules = [AutoGP.Callbacks]","category":"page"},{"location":"utils.html#AutoGP.Callbacks","page":"Utilities","title":"AutoGP.Callbacks","text":"Utilities for creating callbacks to inspect SMC inference.\n\n\n\n\n\n","category":"module"},{"location":"utils.html#AutoGP.Callbacks.make_smc_callback-Tuple{Function, AutoGP.GPModel}","page":"Utilities","title":"AutoGP.Callbacks.make_smc_callback","text":"make_smc_callback(fn::Function, model::AutoGP.GPModel; kwargs...)\n\nConvert fn into a callback for AutoGP.fit_smc!(model, ...).\n\nThe function fn must have a signature of the form fn(; [<opt>,] kw...), where kw... is a varargs specifier and <opt> denotes a (possibly empty) collection of required and optional keyword arguments.\n\nFor example  fn(; a, b=2, kw...) is valid because fn takes no named arguments and the varargs specifier kw... is present. If fn includes named keyword arguments (i.e. a and b), then all required keyword arguments (i.e., a) must be provided in the call to make_smc_callback and optional keyword arguments (i.e., b) may be provided, as shown in the examples below.\n\nmodel = AutoGP.GPModel(...)\nfn = (; a, b=2, kw...) -> ...\nmake_smc_callback(fn, model; a=2)       # valid, `a` specified, `b` optional.\nmake_smc_callback(fn, model; a=1, b=2)  # valid, `a` and `b` are specified.\nmake_smc_callback(fn, model)            # invalid, `a` required but not specified.\nmake_smc_callback(fn, model; b=2)       # invalid, `a` required but not specified.\n\nThe callback return by make_smc_callback is guaranteed to receive in its varargs (which we called kw) an \"SMC\" state which. The following variables can be accessed in the body of fn by indexing kw:\n\nkw[:step]::Integer: Current SMC step.\nkw[:model]::AutoGP.GPModel: Inferred model at current SMC step.\nkw[:ds_next]::AutoGP.IndexType: Future ds (time points) to be observed in later SMC rounds.\nkw[:y_next]::Vector{<:Real}:  Future y (observations) to be observed in later SMC rounds.\nkw[:rejuvenated]::Bool: Did rejuvenation occur at current step?\nkw[:resampled]:Bool: Did resampling occur at current step?\nkw[:elapsed]::Float64: Wall-clock inference runtime elapsed.\nkw[:verbose]::Bool: Verbose setting.\nkw[:schedule]::Vector{Integer}: The SMC data annealing schedule.\nkw[:permutation]::Vector{Integer}: Indexes used to shuffle (model.ds, model.y).\n\n\n\n\n\n","category":"method"},{"location":"utils.html#AutoGP.Schedule","page":"Utilities","title":"AutoGP.Schedule","text":"","category":"section"},{"location":"utils.html","page":"Utilities","title":"Utilities","text":"Modules = [AutoGP.Schedule]","category":"page"},{"location":"utils.html#AutoGP.Schedule","page":"Utilities","title":"AutoGP.Schedule","text":"Utilities for creating SMC annealing schedules, used for AutoGP.fit_smc!.\n\n\n\n\n\n","category":"module"},{"location":"utils.html#AutoGP.Schedule.linear_schedule-Tuple{Integer, Float64}","page":"Utilities","title":"AutoGP.Schedule.linear_schedule","text":"linear_schedule(n::Integer, percent::Float64)\n\nAdds roughly n⋅percent new observations at each step.\n\n\n\n\n\n","category":"method"},{"location":"utils.html#AutoGP.Schedule.logarithmic_schedule-Tuple{Integer, Real, Integer}","page":"Utilities","title":"AutoGP.Schedule.logarithmic_schedule","text":"logarithmic_schedule(n::Integer, base::Integer, start::Integer)\n\nThe first step adds start observations (must be positive). At step i, start⋅baseⁱ new observations are added.\n\n\n\n\n\n","category":"method"},{"location":"utils.html#AutoGP.Schedule.logarithmic_schedule-Tuple{Integer, Real}","page":"Utilities","title":"AutoGP.Schedule.logarithmic_schedule","text":"logarithmic_schedule(n::Integer, base::Real)\n\nThe total number of observations at step i is baseⁱ\n\n\n\n\n\n","category":"method"},{"location":"utils.html#AutoGP.Transforms","page":"Utilities","title":"AutoGP.Transforms","text":"","category":"section"},{"location":"utils.html","page":"Utilities","title":"Utilities","text":"Modules = [AutoGP.Transforms]","category":"page"},{"location":"utils.html#AutoGP.Transforms","page":"Utilities","title":"AutoGP.Transforms","text":"Utilities for creating transformations.\n\n\n\n\n\n","category":"module"},{"location":"utils.html#AutoGP.Transforms.LinearTransform-Tuple{Vector{<:Real}, Any, Any}","page":"Utilities","title":"AutoGP.Transforms.LinearTransform","text":"LinearTransform(data::Vector{<:Real}, lo, hi)\n\nTransform such that minimum(data) = lo and maximum(data) = hi.\n\n\n\n\n\n","category":"method"},{"location":"utils.html#AutoGP.Transforms.LinearTransform-Tuple{Vector{<:Real}, Any}","page":"Utilities","title":"AutoGP.Transforms.LinearTransform","text":"LinearTransform(data::Vector{<:Real}, width)\n\nTransform such that mean(data) = 0 and data is within [-width, width].\n\n\n\n\n\n","category":"method"},{"location":"tutorials/callbacks.html#Inspecting-Online-Inference","page":"Inspecting Online Inference","title":"Inspecting Online Inference","text":"","category":"section"},{"location":"tutorials/callbacks.html","page":"Inspecting Online Inference","title":"Inspecting Online Inference","text":"The goal of this tutorial is to demonstrate [callback functions](https://en.wikipedia.org/wiki/Callback(computerprogramming) for inspecting intermediate results in the sequential Monte Carlo loop of AutoGP.fit_smc!. These callback functions can be used to inspect how the forecasts evolve as more data is incorporated in the model, obtain runtime versus accuracy profiles, and debug poorly performing inference, among many other use cases.","category":"page"},{"location":"tutorials/callbacks.html","page":"Inspecting Online Inference","title":"Inspecting Online Inference","text":"import AutoGP\nimport CSV\nimport Dates\nimport DataFrames\nimport Distributions\n\nusing AutoGP.GP\nusing PyPlot: plt","category":"page"},{"location":"tutorials/callbacks.html","page":"Inspecting Online Inference","title":"Inspecting Online Inference","text":"We begin by using functions from the AutoGP.GP module to generate synthetic data from a ground-truth Gaussian process time series model.","category":"page"},{"location":"tutorials/callbacks.html","page":"Inspecting Online Inference","title":"Inspecting Online Inference","text":"AutoGP.seed!(2)\ncov = (Linear(1,0,50) * Periodic(5,2)) + GammaExponential(1,1)\nnoise = .1\nn = 100\n\nds = Vector{Float64}(range(0, 20, length=n))\ndist = Distributions.MvNormal(cov, noise, Float64[], Float64[], ds)\ny = Distributions.rand(dist);\n\nfig, ax = plt.subplots(figsize=(6, 4), dpi=100, tight_layout=true)\nax.plot(ds, y, marker=\".\", markerfacecolor=\"none\", markeredgecolor=\"k\", color=\"black\");","category":"page"},{"location":"tutorials/callbacks.html","page":"Inspecting Online Inference","title":"Inspecting Online Inference","text":"(Image: png)","category":"page"},{"location":"tutorials/callbacks.html","page":"Inspecting Online Inference","title":"Inspecting Online Inference","text":"We next initialize an AutoGP.GPModel instance with 8 particles. Recall that initializing a model does not fit it to the data.","category":"page"},{"location":"tutorials/callbacks.html","page":"Inspecting Online Inference","title":"Inspecting Online Inference","text":"model = AutoGP.GPModel(ds, y; n_particles=8);","category":"page"},{"location":"tutorials/callbacks.html","page":"Inspecting Online Inference","title":"Inspecting Online Inference","text":"We next define a callback that will be called at the end of each SMC inference. The callback function below, called fn, must take only keyword arguments (e.g., axes), as well as a varags specifier (kwargs...).  At each SMC step, the callback will be invoked and kwargs will be populated with various quantities, as documented in AutoGP.Callbacks.make_smc_callback.  Some quantities used below include kwargs[:step], which is the current SMC step; kwargs[:elapsed], the total runtime in seconds; kwargs[:model], which is the inferred GPModel at the current SMC step; kwargs[:ds_next] and kwargs[:y_next], which are data that will be incorporated future SMC steps.  The observed data so far can be accessed via (kwargs[:model].ds, kwargs[:model].y).","category":"page"},{"location":"tutorials/callbacks.html","page":"Inspecting Online Inference","title":"Inspecting Online Inference","text":"In order to generate plots at each step of inference, the signature of fn includes a parameter called axes, which is expected to be a Dict whose keys are integers (SMC steps) and values are PyPlot.Axes objects.  We will plot the in-sample and out-of-sample predictions at each step.","category":"page"},{"location":"tutorials/callbacks.html","page":"Inspecting Online Inference","title":"Inspecting Online Inference","text":"function fn(; axes::Dict, kwargs...)\n    # Obtain SMC step, current model, and data to-be observed at later SMC steps\n    step = kwargs[:step]\n    model_step = kwargs[:model]\n    ds_next = kwargs[:ds_next]\n    y_next = kwargs[:y_next]\n    \n    # Generate predictions on observed data, to-be observed data, and future data.\n    ds_query = vcat(model_step.ds, ds_next, (20:.1:30))\n    predictions = AutoGP.predict(model_step, ds_query; quantiles=[0.025, 0.975])\n    idx_sort = sortperm(ds_query)\n    \n    # Plot the observed data so far.\n    ax = axes[step]\n    ax.scatter(model_step.ds, model_step.y, marker=\"o\", color=\"k\", s=10, label=\"Observed Data\")    \n    if length(model_step.ds) > 0\n        ax.axvline(maximum(model_step.ds), color=\"red\", linestyle=\"--\")\n    end\n\n    # Plot predictions on future data.\n    for i=1:AutoGP.num_particles(model_step)\n        subdf = predictions[predictions.particle.==i,:]\n        ax.plot(subdf[idx_sort,:ds], subdf[idx_sort,:y_mean], linewidth=.5, color=\"k\")\n        ax.fill_between(\n            subdf[idx_sort,:ds],\n            subdf[idx_sort,\"y_0.025\"],\n            subdf[idx_sort,\"y_0.975\"],\n            color=\"tab:green\",\n            alpha=.05)\n    end\n    ax.legend(loc=\"upper left\")\n    ax.set_title(\"Step $(step), Obs $(length(model_step.ds)), Elapsed $(kwargs[:elapsed])\")\n    ax.set_ylim([-100, 100])\nend","category":"page"},{"location":"tutorials/callbacks.html","page":"Inspecting Online Inference","title":"Inspecting Online Inference","text":"fn (generic function with 1 method)","category":"page"},{"location":"tutorials/callbacks.html","page":"Inspecting Online Inference","title":"Inspecting Online Inference","text":"Now that we have defined the callback fn we prepare the overall figure and axes dictionary.  To create the actual callback that will be provided to AutoGP.fit_smc! we use AutoGP.Callbacks.make_smc_callback.","category":"page"},{"location":"tutorials/callbacks.html","page":"Inspecting Online Inference","title":"Inspecting Online Inference","text":"note: Note\nThe model argument in the call to make_smc_callback is different than kwargs[:model] in the body of the callback fn.  The former model is obtained by initializing (but not fit fitting) AutoGP.GPModel on the full data in the global scope, whereas kwargs[:model] is a temporary GPModel that has been fitted to the observed data so far.","category":"page"},{"location":"tutorials/callbacks.html","page":"Inspecting Online Inference","title":"Inspecting Online Inference","text":"# Prepare the SMC schedule and intermediate plots\nschedule = AutoGP.Schedule.linear_schedule(n, .05)\nnrows = 1 + length(schedule)\nfig, axes = plt.subplots(nrows=nrows, figsize=(6, 4*nrows), tight_layout=true, dpi=100)\naxes = Dict(0=>axes[1], (step=>ax for (step, ax) in zip(schedule, axes[2:end]))...)\n\n# Make the callback function.\ncallback_fn = AutoGP.Callbacks.make_smc_callback(fn, model; axes=axes)\n\n# Perform inference.\nAutoGP.fit_smc!(model; schedule=schedule, n_mcmc=10, n_hmc=10, shuffle=false, callback_fn=callback_fn);","category":"page"},{"location":"tutorials/callbacks.html","page":"Inspecting Online Inference","title":"Inspecting Online Inference","text":"(Image: png)","category":"page"},{"location":"tutorials/callbacks.html","page":"Inspecting Online Inference","title":"Inspecting Online Inference","text":"for k in AutoGP.covariance_kernels(model);\n    display(k)\nend","category":"page"},{"location":"tutorials/callbacks.html","page":"Inspecting Online Inference","title":"Inspecting Online Inference","text":"×\n├── ＋\n│   ├── GE(2.09, 1.32; 0.25)\n│   └── PER(0.57, 0.20; 0.23)\n└── ×\n    ├── LIN(0.16; 0.11, 0.04)\n    └── LIN(0.10; 0.28, 1.18)\n\n\n\n\n×\n├── ＋\n│   ├── GE(2.09, 1.32; 0.25)\n│   └── PER(0.57, 0.20; 0.23)\n└── ×\n    ├── LIN(0.16; 0.11, 0.04)\n    └── LIN(0.10; 0.28, 1.18)\n\n\n\n\n×\n├── ＋\n│   ├── GE(2.09, 1.32; 0.25)\n│   └── PER(0.57, 0.20; 0.23)\n└── ×\n    ├── LIN(0.16; 0.11, 0.04)\n    └── LIN(0.10; 0.28, 1.18)\n\n\n\n\n×\n├── ＋\n│   ├── GE(2.09, 1.32; 0.25)\n│   └── PER(0.57, 0.20; 0.23)\n└── ×\n    ├── LIN(0.16; 0.11, 0.04)\n    └── LIN(0.10; 0.28, 1.18)\n\n\n\n\n×\n├── ＋\n│   ├── GE(2.09, 1.32; 0.25)\n│   └── PER(0.57, 0.20; 0.23)\n└── ×\n    ├── LIN(0.12; 0.19, 0.40)\n    └── LIN(0.10; 0.28, 1.18)\n\n\n\n\n×\n├── ＋\n│   ├── GE(5.19, 1.38; 0.62)\n│   └── PER(0.57, 0.20; 0.23)\n└── ×\n    ├── LIN(0.12; 0.19, 0.40)\n    └── LIN(0.06; 0.16, 0.44)\n\n\n\n\n×\n├── ＋\n│   ├── GE(2.09, 1.32; 0.25)\n│   └── PER(0.57, 0.20; 0.23)\n└── ×\n    ├── LIN(0.12; 0.19, 0.40)\n    └── LIN(0.06; 0.16, 0.44)\n\n\n\n\n×\n├── ＋\n│   ├── GE(2.09, 1.32; 0.25)\n│   └── PER(0.57, 0.20; 0.23)\n└── ×\n    ├── LIN(0.12; 0.19, 0.40)\n    └── LIN(0.10; 0.28, 1.18)","category":"page"}]
}
