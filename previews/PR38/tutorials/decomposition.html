<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Time Series Decomposition · AutoGP</title><meta name="title" content="Time Series Decomposition · AutoGP"/><meta property="og:title" content="Time Series Decomposition · AutoGP"/><meta property="twitter:title" content="Time Series Decomposition · AutoGP"/><meta name="description" content="Documentation for AutoGP."/><meta property="og:description" content="Documentation for AutoGP."/><meta property="twitter:description" content="Documentation for AutoGP."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../index.html"><img src="../assets/logo.svg" alt="AutoGP logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../index.html">AutoGP</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../index.html">Home</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="overview.html">Overview</a></li><li><a class="tocitem" href="iclaims.html">Insurance Claims</a></li><li><a class="tocitem" href="callbacks.html">Inspecting Online Inference</a></li><li class="is-active"><a class="tocitem" href="decomposition.html">Time Series Decomposition</a><ul class="internal"><li><a class="tocitem" href="#Hierarchical-Decomposition-of-Kernels"><span>Hierarchical Decomposition of Kernels</span></a></li><li><a class="tocitem" href="#STL-Style-Decomposition"><span>STL Style Decomposition</span></a></li><li><a class="tocitem" href="#Sum-of-Products-Decomposition"><span>Sum-of-Products Decomposition</span></a></li></ul></li><li><a class="tocitem" href="greedy_mcmc.html">Greedy Search and MCMC</a></li></ul></li><li><a class="tocitem" href="../api.html">AutoGP API</a></li><li><a class="tocitem" href="../gp.html">Gaussian Process Library</a></li><li><a class="tocitem" href="../utils.html">Utilities</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href="decomposition.html">Time Series Decomposition</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="decomposition.html">Time Series Decomposition</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/probsys/AutoGP.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/probsys/AutoGP.jl/blob/main/docs/src/tutorials/decomposition.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Time-Series-Decomposition"><a class="docs-heading-anchor" href="#Time-Series-Decomposition">Time Series Decomposition</a><a id="Time-Series-Decomposition-1"></a><a class="docs-heading-anchor-permalink" href="#Time-Series-Decomposition" title="Permalink"></a></h1><p>This tutorial shows how to decompose AutoGP models into their constituent temporal components, to gain more insight into the learned time series structures.</p><p>There are many ways to decompose covariance kernels. We will demonstrate three methods:</p><ul><li><a href="../api.html#AutoGP.decompose"><code>AutoGP.decompose</code></a>. This function breaks down a composite kernel into the constituent subkernels in the expression tree.</li></ul><ul><li><p><a href="../api.html#AutoGP.extract_kernel"><code>AutoGP.extract_kernel</code></a>. This function extracts a specific primitive kernel from a composite kernel, while discarding the others.</p></li><li><p><a href="../api.html#AutoGP.split_kernel_sop"><code>AutoGP.split_kernel_sop</code></a>. This function splits a kernel into a pair of kernels through a sum-of-products interpretation.</p></li></ul><pre><code class="language-julia hljs">import AutoGP</code></pre><pre><code class="language-julia hljs">using CSV
using Dates
using DataFrames
using PythonPlot</code></pre><pre><code class="language-julia hljs">AutoGP.seed!(10)</code></pre><pre><code class="language-julia hljs">data = CSV.File(&quot;assets/M1266.csv&quot;)
M3 = DataFrame(data);
df = M3[:,[&quot;ds&quot;,&quot;y&quot;]];</code></pre><p>We next split the data into a training set and test set.</p><pre><code class="language-julia hljs">n_test = 18
n_train = DataFrames.nrow(df) - n_test
df_train = df[1:end-n_test, :]
df_test = df[end-n_test+1:end, :]

fig, ax = PythonPlot.subplots(figsize=(10,4))
ax.scatter(df_train.ds, df_train.y, marker=&quot;o&quot;, color=&quot;k&quot;, alpha=.5)
ax.scatter(df_test.ds, df_test.y, marker=&quot;o&quot;, color=&quot;w&quot;, edgecolor=&quot;k&quot;, label=&quot;Test Data&quot;)</code></pre><p><img src="decomposition_files/decomposition_7_0.png" alt="png"/></p><pre><code class="nohighlight hljs">Python: &lt;matplotlib.collections.PathCollection object at 0x713940142a80&gt;</code></pre><pre><code class="language-julia hljs">model = AutoGP.GPModel(df_train.ds, df_train.y; n_particles=18);</code></pre><pre><code class="language-julia hljs">ds_future = range(start=df.ds[end]+Dates.Month(1), step=Dates.Month(1), length=4*size(df_test)[1])
ds_query = vcat(df_train.ds, df_test.ds, ds_future)
forecasts = AutoGP.predict(model, ds_query; quantiles=[0.025, 0.975]);</code></pre><pre><code class="language-julia hljs">AutoGP.fit_smc!(model; schedule=vcat(collect(range(2, n_train, step=12)), n_train), n_mcmc=100, n_hmc=20, verbose=false);</code></pre><pre><code class="language-julia hljs">forecasts = AutoGP.predict(model, ds_query; quantiles=[0.025, 0.975]);</code></pre><pre><code class="language-julia hljs">fig, ax = PythonPlot.subplots(figsize=(10,4))
for i=1:AutoGP.num_particles(model)
    subdf = forecasts[forecasts.particle.==i,:]
    ax.plot(subdf[!,&quot;ds&quot;], subdf[!,&quot;y_mean&quot;], color=&quot;k&quot;, linewidth=.5)
    ax.fill_between(
        subdf.ds, subdf[!,&quot;y_0.025&quot;], subdf[!,&quot;y_0.975&quot;];
        color=&quot;tab:blue&quot;, alpha=0.05)
end
ax.scatter(df_train.ds, df_train.y, marker=&quot;o&quot;, color=&quot;k&quot;, label=&quot;Observed Data&quot;)
ax.scatter(df_test.ds, df_test.y, marker=&quot;o&quot;, color=&quot;w&quot;, edgecolor=&quot;k&quot;, label=&quot;Test Data&quot;)</code></pre><p><img src="decomposition_files/decomposition_12_0.png" alt="png"/></p><pre><code class="nohighlight hljs">Python: &lt;matplotlib.collections.PathCollection object at 0x7138acd17d10&gt;</code></pre><h2 id="Hierarchical-Decomposition-of-Kernels"><a class="docs-heading-anchor" href="#Hierarchical-Decomposition-of-Kernels">Hierarchical Decomposition of Kernels</a><a id="Hierarchical-Decomposition-of-Kernels-1"></a><a class="docs-heading-anchor-permalink" href="#Hierarchical-Decomposition-of-Kernels" title="Permalink"></a></h2><p>Let us first inspect the learned kernels.</p><pre><code class="language-julia hljs">weights = AutoGP.particle_weights(model)
kernels = AutoGP.covariance_kernels(model)
for (i, (k, w)) in enumerate(zip(kernels, weights))
    println(&quot;Model $(i), Weight $(w)&quot;)
    display(k)
end</code></pre><pre><code class="nohighlight hljs">Model 1, Weight 0.00901154857652315



×
├── +
│   ├── LIN(659981029.03; 97715.80, 0.00)
│   └── LIN(1195212711.94; 560685.09, 0.00)
└── +
    ├── +
    │   ├── ×
    │   │   ├── ×
    │   │   │   ├── ×
    │   │   │   │   ├── LIN(645756746.98; 0.12, 0.00)
    │   │   │   │   └── LIN(456367123.42; 0.16, 0.00)
    │   │   │   └── LIN(567802575.74; 0.04, 0.00)
    │   │   └── GE(74314112.16, 1.31; 0.38)
    │   └── PER(1.74, 31698075.82; 0.14)
    └── LIN(435409162.25; 0.10, 0.00)



Model 2, Weight 0.019612029584188975



×
├── LIN(553528205.92; 193073.36, 0.00)
└── +
    ├── GE(610350376.93, 1.32; 0.44)
    └── PER(1.48, 31339381.01; 0.17)



Model 3, Weight 0.017007994352312553



+
├── ×
│   ├── ×
│   │   ├── LIN(466187204.31; 254341.57, 0.00)
│   │   └── LIN(883086760.04; 0.22, 0.00)
│   └── +
│       ├── +
│       │   ├── ×
│       │   │   ├── GE(72266269.78, 1.63; 0.14)
│       │   │   └── LIN(444058228.24; 0.06, 0.00)
│       │   └── +
│       │       ├── LIN(724687893.47; 0.08, 0.00)
│       │       └── LIN(560627320.97; 0.75, 0.00)
│       └── PER(1.09, 31585406.18; 0.13)
└── LIN(558190191.81; 4095788.70, 0.00)



Model 4, Weight 0.11069109354255227



+
├── +
│   ├── GE(566952789.69, 1.66; 53807.36)
│   └── PER(1.86, 31493709.45; 126071.89)
└── LIN(506782708.52; 24208.61, 0.00)



Model 5, Weight 0.015355520708968496



+
├── ×
│   ├── ×
│   │   ├── LIN(703521266.93; 402680.15, 0.00)
│   │   └── LIN(474054605.17; 0.29, 0.00)
│   └── +
│       ├── ×
│       │   ├── LIN(477789221.18; 0.08, 0.00)
│       │   └── +
│       │       ├── GE(93852771.12, 0.56; 0.04)
│       │       └── LIN(445472740.93; 0.11, 0.00)
│       └── PER(1.43, 31593348.89; 0.18)
└── LIN(559007522.80; 27079.33, 0.00)



Model 6, Weight 0.02878043770753873



+
├── ×
│   ├── ×
│   │   ├── LIN(455085567.29; 199889.05, 0.00)
│   │   └── LIN(905419786.45; 0.23, 0.00)
│   └── +
│       ├── ×
│       │   ├── LIN(640199044.16; 0.06, 0.00)
│       │   └── GE(100589877.05, 1.25; 0.18)
│       └── PER(1.43, 31593348.89; 0.18)
└── LIN(593954212.76; 301975.24, 0.00)



Model 7, Weight 0.014477329933002096



+
├── ×
│   ├── +
│   │   ├── LIN(497587817.58; 639968.07, 0.00)
│   │   └── LIN(471057791.81; 1253031.98, 0.00)
│   └── +
│       ├── +
│       │   ├── ×
│       │   │   ├── ×
│       │   │   │   ├── ×
│       │   │   │   │   ├── LIN(458436935.16; 0.16, 0.00)
│       │   │   │   │   └── +
│       │   │   │   │       ├── LIN(610953981.25; 0.16, 0.00)
│       │   │   │   │       └── ×
│       │   │   │   │           ├── ×
│       │   │   │   │           │   ├── LIN(478408274.62; 0.04, 0.00)
│       │   │   │   │           │   └── +
│       │   │   │   │           │       ├── LIN(574886530.87; 0.32, 0.00)
│       │   │   │   │           │       └── LIN(460056017.66; 0.30, 0.00)
│       │   │   │   │           └── ×
│       │   │   │   │               ├── +
│       │   │   │   │               │   ├── +
│       │   │   │   │               │   │   ├── LIN(667199694.27; 0.39, 0.00)
│       │   │   │   │               │   │   └── ×
│       │   │   │   │               │   │       ├── GE(43654821.17, 1.55; 0.30)
│       │   │   │   │               │   │       └── +
│       │   │   │   │               │   │           ├── LIN(559025780.12; 0.03, 0.00)
│       │   │   │   │               │   │           └── ×
│       │   │   │   │               │   │               ├── +
│       │   │   │   │               │   │               │   ├── PER(0.14, 80045439.57; 0.18)
│       │   │   │   │               │   │               │   └── +
│       │   │   │   │               │   │               │       ├── +
│       │   │   │   │               │   │               │       │   ├── LIN(539688999.60; 0.20, 0.00)
│       │   │   │   │               │   │               │       │   └── PER(0.19, 19029757.25; 0.04)
│       │   │   │   │               │   │               │       └── ×
│       │   │   │   │               │   │               │           ├── ×
│       │   │   │   │               │   │               │           │   ├── PER(0.12, 77479085.99; 0.07)
│       │   │   │   │               │   │               │           │   └── PER(0.18, 144355664.13; 0.01)
│       │   │   │   │               │   │               │           └── GE(44444499.68, 0.71; 0.35)
│       │   │   │   │               │   │               └── GE(27066775.64, 1.55; 0.13)
│       │   │   │   │               │   └── GE(25937540.58, 1.13; 0.03)
│       │   │   │   │               └── PER(0.29, 30436188.75; 0.13)
│       │   │   │   └── GE(360706174.03, 1.63; 0.04)
│       │   │   └── GE(54699037.16, 1.18; 0.23)
│       │   └── PER(2.03, 31575590.50; 0.15)
│       └── LIN(578615206.91; 0.97, 0.00)
└── LIN(450947315.34; 756380.09, 0.00)



Model 8, Weight 0.047447926279920595



+
├── +
│   ├── GE(490829705.72, 1.61; 53891.92)
│   └── PER(1.86, 31474242.59; 140799.04)
└── +
    ├── +
    │   ├── LIN(719928809.88; 140065.50, 0.00)
    │   └── LIN(450286397.01; 73853.58, 0.00)
    └── LIN(425778979.01; 283629.75, 0.00)



Model 9, Weight 0.010109195479711725



×
├── LIN(510605573.28; 538468.10, 0.00)
└── +
    ├── ×
    │   ├── +
    │   │   ├── ×
    │   │   │   ├── ×
    │   │   │   │   ├── GE(73592603.05, 1.32; 0.30)
    │   │   │   │   └── GE(435853766.92, 0.97; 0.18)
    │   │   │   └── LIN(578736277.07; 0.10, 0.00)
    │   │   └── LIN(485369591.57; 0.50, 0.00)
    │   └── LIN(571286497.84; 0.34, 0.00)
    └── +
        ├── LIN(472756425.93; 0.15, 0.00)
        └── PER(1.29, 31481465.66; 0.16)



Model 10, Weight 0.00019137991483466764



×
├── LIN(568090873.20; 139939.21, 0.00)
└── +
    ├── ×
    │   ├── GE(817646585.97, 1.40; 0.50)
    │   └── +
    │       ├── LIN(444799222.44; 0.03, 0.00)
    │       └── LIN(450268972.65; 0.28, 0.00)
    └── PER(1.24, 31953635.36; 0.17)



Model 11, Weight 0.0054314969855522



+
├── ×
│   ├── ×
│   │   ├── LIN(568363450.73; 442906.88, 0.00)
│   │   └── LIN(478193065.57; 0.35, 0.00)
│   └── +
│       ├── GE(129762049.27, 1.64; 0.07)
│       └── PER(1.42, 31677669.77; 0.25)
└── LIN(479392279.90; 35663.69, 0.00)



Model 12, Weight 0.0076299623510348



+
├── ×
│   ├── LIN(794821416.44; 95231.27, 0.00)
│   └── +
│       ├── GE(212766080.39, 1.53; 0.08)
│       └── PER(1.28, 31642882.88; 0.21)
└── ×
    ├── LIN(1501215005.41; 111622.20, 0.00)
    └── LIN(551494152.52; 0.15, 0.00)



Model 13, Weight 0.3270158643889261



+
├── +
│   ├── GE(308405091.32, 1.55; 46032.60)
│   └── PER(1.86, 31474242.59; 140799.04)
└── LIN(478990944.11; 87109.72, 0.00)



Model 14, Weight 0.08141232858111867



+
├── ×
│   ├── LIN(468516482.12; 1517671.74, 0.00)
│   └── +
│       ├── ×
│       │   ├── ×
│       │   │   ├── ×
│       │   │   │   ├── LIN(458383824.50; 0.08, 0.00)
│       │   │   │   └── LIN(490260027.05; 0.05, 0.00)
│       │   │   └── GE(490829705.72, 1.61; 0.05)
│       │   └── GE(67306478.43, 1.19; 0.35)
│       └── PER(1.86, 31474242.59; 0.14)
└── LIN(611309629.23; 173952.01, 0.00)



Model 15, Weight 0.0005021571370656624



+
├── +
│   ├── ×
│   │   ├── LIN(609809419.35; 610124.73, 0.00)
│   │   └── +
│   │       ├── +
│   │       │   ├── ×
│   │       │   │   ├── ×
│   │       │   │   │   ├── GE(78117685.74, 1.58; 0.04)
│   │       │   │   │   └── LIN(617136973.20; 0.35, 0.00)
│   │       │   │   └── LIN(556041942.24; 0.27, 0.00)
│   │       │   └── LIN(1202344796.78; 0.20, 0.00)
│   │       └── PER(1.91, 31684257.18; 0.22)
│   └── LIN(424034379.19; 103118.46, 0.00)
└── LIN(468608844.61; 212589.52, 0.00)



Model 16, Weight 0.2575040599217603



+
├── +
│   ├── GE(490829705.72, 1.61; 53891.92)
│   └── PER(1.86, 31474242.59; 140799.04)
└── LIN(527880879.77; 307350.63, 0.00)



Model 17, Weight 0.027518336753961727



×
├── LIN(468175029.06; 113767.87, 0.00)
└── +
    ├── +
    │   ├── LIN(525326031.11; 0.19, 0.00)
    │   └── GE(320816202.09, 1.73; 0.99)
    └── +
        ├── LIN(556245068.01; 0.17, 0.00)
        └── PER(1.40, 31519222.30; 0.21)



Model 18, Weight 0.020301337801024034



+
├── GE(361385655.65, 1.74; 365081.64)
└── PER(1.30, 31583405.26; 131132.44)</code></pre><p>We now use <a href="../api.html#AutoGP.decompose"><code>AutoGP.decompose</code></a> to hierarchically break down the composite kernel into all the constituent subkernels.</p><pre><code class="language-julia hljs">decomposed_models = AutoGP.decompose(model);</code></pre><pre><code class="language-julia hljs"># Helper function to pretty print covariance.
function show_string(x)
    io = IOBuffer()
    Base.show(io, MIME(&quot;text/plain&quot;), x)
    return String(take!(io))
end

# Plot the decomposition of `model` into its constituent parts.
function plot_decomposition(model::AutoGP.GPModel)
    kernels = AutoGP.covariance_kernels(model)
    forecasts = AutoGP.predict(
        model, ds_query;
        quantiles=[0.025, 0.975]);
    fig, axes = PythonPlot.subplots(
        nrows=AutoGP.num_particles(model),
        ncols=2,
        tight_layout=true,
        figsize=(12, 6*AutoGP.num_particles(model)),
        )
    for i=1:AutoGP.num_particles(model)
        subdf = forecasts[forecasts.particle.==i,:]
        # axes[i].set_title(show_string(kernels[i]), ha=&quot;left&quot;)
        axes[i-1,0].plot(subdf[!,&quot;ds&quot;], subdf[!,&quot;y_mean&quot;], color=&quot;k&quot;, linewidth=1, label=show_string(kernels[i]))
        axes[i-1,0].fill_between(
            subdf.ds, subdf[!,&quot;y_0.025&quot;], subdf[!,&quot;y_0.975&quot;];
            color=&quot;tab:blue&quot;, alpha=0.05)
        axes[i-1,0].scatter(df_train.ds, df_train.y, marker=&quot;o&quot;, color=&quot;k&quot;, label=&quot;Observed Data&quot;)
        axes[i-1,0].scatter(df_test.ds, df_test.y, marker=&quot;o&quot;, color=&quot;w&quot;, edgecolor=&quot;k&quot;, label=&quot;Test Data&quot;)
        axes[i-1,1].text(0.5, 0.5, show_string(kernels[i]), transform=axes[i-1,1].transAxes,  va=&quot;center&quot;, ha=&quot;left&quot;)
        axes[i-1,1].set_axis_off()
    end
    return fig, axes
end</code></pre><pre><code class="nohighlight hljs">plot_decomposition (generic function with 1 method)</code></pre><p>Let us plot the decomposition of a given particle in the ensemble.</p><pre><code class="language-julia hljs">idx = 13
fig, ax = plot_decomposition(decomposed_models[11]);
fig.suptitle(&quot;Decomposition of Learned Model $(idx)&quot;, fontsize=18, va=&quot;center&quot;, y=1);</code></pre><p><img src="decomposition_files/decomposition_20_0.png" alt="png"/></p><h2 id="STL-Style-Decomposition"><a class="docs-heading-anchor" href="#STL-Style-Decomposition">STL Style Decomposition</a><a id="STL-Style-Decomposition-1"></a><a class="docs-heading-anchor-permalink" href="#STL-Style-Decomposition" title="Permalink"></a></h2><p>An alternative approach to decomposing kernels is using <a href="../api.html#AutoGP.extract_kernel"><code>AutoGP.extract_kernel</code></a>, which retains only a specific primitive kernel while discarding the others. In the following example, we will extract the <a href="../gp.html#AutoGP.GP.Linear"><code>AutoGP.GP.Linear</code></a>, <a href="../gp.html#AutoGP.GP.Periodic"><code>AutoGP.GP.Periodic</code></a>, and <a href="../gp.html#AutoGP.GP.GammaExponential"><code>AutoGP.GP.GammaExponential</code></a> kernels from each learned particle to produce an &quot;STL&quot; style decomposition.</p><pre><code class="language-julia hljs">model_per = AutoGP.extract_kernel(model, AutoGP.GP.Periodic);
model_ge = AutoGP.extract_kernel(model, AutoGP.GP.GammaExponential);
model_lin = AutoGP.extract_kernel(model, AutoGP.GP.Linear);</code></pre><p>Let us study the original and decomposed kernels for a given particle.</p><p>Unlike a <a href="https://en.wikipedia.org/wiki/Decomposition_of_time_series">traditional time series decomposition</a>, which typically assumes a fixed additive or multiplicative structure, these decompositions retain the learned structure. For example, the decomposition for <code>Linear</code> may have a quadratic term, if the overall kernel has a subexpression of the form <code>LIN * LIN</code>.</p><p>The kernel structure is retained by using the <a href="../gp.html#AutoGP.GP.Constant"><code>AutoGP.GP.Constant</code></a> to act as a &quot;noop&quot;, as shown below. See also <a href="../api.html#AutoGP.extract_kernel"><code>AutoGP.extract_kernel</code></a> for full details.</p><pre><code class="language-julia hljs">idx = 2
println(&quot;Model $(idx) - FULL&quot;); display(AutoGP.covariance_kernels(model)[2])
println(&quot;Model $(idx) - LIN only&quot;); display(AutoGP.covariance_kernels(model_lin)[2])
println(&quot;Model $(idx) - PER only&quot;); display(AutoGP.covariance_kernels(model_per)[2])
println(&quot;Model $(idx) - GE only&quot;); display(AutoGP.covariance_kernels(model_ge)[2])</code></pre><pre><code class="nohighlight hljs">Model 2 - FULL



×
├── LIN(553528205.92; 193073.36, 0.00)
└── +
    ├── GE(610350376.93, 1.32; 0.44)
    └── PER(1.48, 31339381.01; 0.17)



Model 2 - LIN only



×
├── LIN(553528205.92; 193073.36, 0.00)
└── +
    ├── CONST(0.00)
    └── CONST(0.00)



Model 2 - PER only



×
├── CONST(985751.12)
└── +
    ├── CONST(0.00)
    └── PER(1.48, 31339381.01; 0.17)



Model 2 - GE only



×
├── CONST(985751.12)
└── +
    ├── GE(610350376.93, 1.32; 0.44)
    └── CONST(0.00)</code></pre><p>We can now obtain forecasts corresponding to the Linear, Periodic, and GammaExponential components in each particle.</p><pre><code class="language-julia hljs">forecasts_lin = AutoGP.predict(model_lin, ds_query .+ Day(1); quantiles=[0.025, 0.975]);
forecasts_per = AutoGP.predict(model_per, ds_query .+ Day(1); quantiles=[0.025, 0.975]);
forecasts_ge = AutoGP.predict(model_ge, ds_query .+ Day(1); quantiles=[0.025, 0.975]);</code></pre><pre><code class="language-julia hljs">fig, axes = PythonPlot.subplots(figsize=(10,14), nrows=3, tight_layout=true)
for (ax, m, f) in zip(axes, [model_lin, model_per, model_ge], [forecasts_lin, forecasts_per, forecasts_ge])
    for i=1:AutoGP.num_particles(m)
        subdf = f[f.particle.==i,:]
        ax.plot(subdf[!,&quot;ds&quot;], subdf[!,&quot;y_mean&quot;], color=&quot;k&quot;, linewidth=.5)
        ax.fill_between(subdf.ds, subdf[!,&quot;y_0.025&quot;], subdf[!,&quot;y_0.975&quot;]; color=&quot;tab:blue&quot;, alpha=0.05)
    end
    ax.scatter(df_train.ds, df_train.y, marker=&quot;o&quot;, color=&quot;k&quot;, label=&quot;Observed Data&quot;)
    ax.scatter(df_test.ds, df_test.y, marker=&quot;o&quot;, color=&quot;w&quot;, edgecolor=&quot;k&quot;, label=&quot;Test Data&quot;)
end
axes[0].set_title(&quot;STRUCTURE: LIN&quot;)
axes[1].set_title(&quot;STRUCTURE: PER&quot;)
axes[2].set_title(&quot;STRUCTURE: GE&quot;)</code></pre><p><img src="decomposition_files/decomposition_28_0.png" alt="png"/></p><pre><code class="nohighlight hljs">Python: Text(0.5, 1.0, &#39;STRUCTURE: GE&#39;)</code></pre><h2 id="Sum-of-Products-Decomposition"><a class="docs-heading-anchor" href="#Sum-of-Products-Decomposition">Sum-of-Products Decomposition</a><a id="Sum-of-Products-Decomposition-1"></a><a class="docs-heading-anchor-permalink" href="#Sum-of-Products-Decomposition" title="Permalink"></a></h2><p>An third approach to decomposing kernels is using <a href="../api.html#AutoGP.split_kernel_sop"><code>AutoGP.split_kernel_sop</code></a>, which is based on a sum-of-products decomposition of kernels.</p><p>In particular, we can write any composite covariance kernel <span>$k$</span> as a sum of <span>$m$</span> products, where the <span>$i$</span>th term in the sum is a product of <span>$n_i$</span> terms:</p><p class="math-container">\[k = k_{11}k_{12}\cdots k_{1n_1} + k_{21}k_{22}\cdots k_{2n_2} + \dots + k_{m1}k_{m2}\cdots k_{m n_m}.\]</p><p>For a given primitive base kernel, such as <code>Periodic</code>, we can rewrite the above expression as</p><p class="math-container">\[k = k^{\rm PER} + k^{\rm NOPER},\]</p><p>where <span>$k^{\rm PER}$</span> contains all addends with a <code>Periodic</code> factor, and <span>$k^{\rm NOPER}$</span> are the addends without a <code>Periodic</code> factor.</p><p>See <a href="../gp.html#AutoGP.GP.split_kernel_sop"><code>AutoGP.GP.split_kernel_sop</code></a> for additional details.</p><p>The following function returns a pair of <a href="../api.html#AutoGP.GPModel"><code>AutoGP.GPModel</code></a> instances based on this decomposition.</p><pre><code class="language-julia hljs">model_a, model_b = AutoGP.split_kernel_sop(model, AutoGP.GP.Periodic);</code></pre><p>Here is an example of the decomposition on the 4th particle of <code>model</code>.</p><pre><code class="language-julia hljs">idx = 4
println(&quot;Model $(idx) - ALL&quot;); display(AutoGP.covariance_kernels(model)[idx])
println(&quot;Model $(idx) - PER&quot;); display(AutoGP.covariance_kernels(model_a)[idx])
println(&quot;Model $(idx) - NO PER&quot;); display(AutoGP.covariance_kernels(model_b)[idx])</code></pre><pre><code class="nohighlight hljs">Model 4 - ALL



+
├── +
│   ├── GE(566952789.69, 1.66; 53807.36)
│   └── PER(1.86, 31493709.45; 126071.89)
└── LIN(506782708.52; 24208.61, 0.00)



Model 4 - PER



PER(1.86, 31493709.45; 126071.89)



Model 4 - NO PER



+
├── GE(566952789.69, 1.66; 53807.36)
└── LIN(506782708.52; 24208.61, 0.00)</code></pre><pre><code class="language-julia hljs">forecasts_a = AutoGP.predict(model_a, ds_query .+ Day(1); quantiles=[0.025, 0.975]);
forecasts_b = AutoGP.predict(model_b, ds_query .+ Day(1); quantiles=[0.025, 0.975]);</code></pre><pre><code class="language-julia hljs">fig, axes = PythonPlot.subplots(figsize=(10,14), nrows=2, tight_layout=true)
for (ax, m, f) in zip(axes, [model_a, model_b], [forecasts_a, forecasts_b])
    for i=1:AutoGP.num_particles(m)
        subdf = f[f.particle.==i,:]
        ax.plot(subdf[!,&quot;ds&quot;], subdf[!,&quot;y_mean&quot;], color=&quot;k&quot;, linewidth=.5)
        ax.fill_between(subdf.ds, subdf[!,&quot;y_0.025&quot;], subdf[!,&quot;y_0.975&quot;]; color=&quot;tab:blue&quot;, alpha=0.05)
    end
    ax.scatter(df_train.ds, df_train.y, marker=&quot;o&quot;, color=&quot;k&quot;, label=&quot;Observed Data&quot;)
    ax.scatter(df_test.ds, df_test.y, marker=&quot;o&quot;, color=&quot;w&quot;, edgecolor=&quot;k&quot;, label=&quot;Test Data&quot;)
end
axes[0].set_title(&quot;STRUCTURE: PER&quot;)
axes[1].set_title(&quot;STRUCTURE: NO PER&quot;)</code></pre><p><img src="decomposition_files/decomposition_36_0.png" alt="png"/></p><pre><code class="nohighlight hljs">Python: Text(0.5, 1.0, &#39;STRUCTURE: NO PER&#39;)</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="callbacks.html">« Inspecting Online Inference</a><a class="docs-footer-nextpage" href="greedy_mcmc.html">Greedy Search and MCMC »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.15.0 on <span class="colophon-date" title="Wednesday 29 October 2025 17:54">Wednesday 29 October 2025</span>. Using Julia version 1.8.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
